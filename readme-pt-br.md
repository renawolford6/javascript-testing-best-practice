<img src="/assets/jtbp-header-blue.png" width="1920px"/>

<br/>

# üëá Por que este guia pode levar suas habilidades de teste para o pr√≥ximo n√≠vel

<br/>

## üìó 45+ boas pr√°ticas: Super abrangente e exaustivo
Este √© um guia para a confiabilidade JavaScript & Node.js da A-Z. Ele resume e organiza para voc√™ dezenas das melhores publica√ß√µes, livros, ferramentas e postagens de blogs que o mercado tem a oferecer


## üö¢ Avan√ßado: vai 10.000 milhas al√©m do b√°sico
Entre em uma jornada que vai muito al√©m do b√°sico, para t√≥picos avan√ßados como testes em produ√ß√£o, testes de muta√ß√£o, testes baseados em propriedades e muitas outras ferramentas estrat√©gicas e profissionais. Se voc√™ ler todas as palavras deste guia, √© prov√°vel que suas habilidades de teste superem a m√©dia


## üåê Full-stack: front, backend, CI(Integra√ß√£o Cont√≠nua), qualquer coisa
Comece entendendo as pr√°ticas de teste onipresentes que s√£o a base para qualquer camada de aplicativo. Em seguida, mergulhe na sua √°rea de escolha: front-end/UI, back-end, CI(Integra√ß√£o Cont√≠nua) ou talvez todos eles?

<br/>

### Escrito por Yoni Goldberg
* Um consultor JavaScript & Node.js
* üë®‚Äçüè´ [Minha oficina de testes](https://www.testjavascript.com) -  aprenda sobre [meus workshops](https://www.testjavascript.com) na Europe & Estados Unidos
* [Me siga no twitter ](https://twitter.com/goldbergyoni/)
* Venha me ouvir falar em [LA](https://js.la/), [Verona](https://2019.nodejsday.it/), [Kharkiv](https://kharkivjs.org/), [free webinar](https://zoom.us/webinar/register/1015657064375/WN_Lzvnuv4oQJOYey2jXNqX6A). Eventos futuros TBD
* [Newsletter informativo de qualidade sobre JavaScript](https://testjavascript.com/newsletter/) - insights e conte√∫do apenas em assuntos estrat√©gicos


<br/><br/>

## `√çndice`

#### [`Se√ß√£o 0: A Regra de ouro`](#section-0Ô∏è‚É£-the-golden-rule)

Um √∫nico conselho que inspira todos os outros (1 marcador especial)

#### [`Se√ß√£o 1: A Anatomia do Teste`](#section-1-the-test-anatomy-1)

A funda√ß√£o - estruturando testes limpos (12 marcadores)

#### [`Se√ß√£o 2: Backend`](#section-2Ô∏è‚É£-backend-testing)

Escrevendo testes de back-end e microsservi√ßos com efici√™ncia (8 marcadores)

#### [`Se√ß√£o 3: Frontend`](#section-3Ô∏è‚É£-frontend-testing)

Escrevendo testes para interface do usu√°rio da web, incluindo testes de componentes e E2E (11 marcadores)

#### [`Se√ß√£o 4: Metrificando Testes Efetivamente`](#section-4Ô∏è‚É£-measuring-test-effectiveness)

Observando o vigia - medindo a qualidade do teste (4 marcadores)

#### [`Se√ß√£o 5: Integra√ß√£o Cont√≠nua`](#section-5Ô∏è‚É£-ci-and-other-quality-measures)

Diretrizes para CI no mundo JS (9 marcadores)


<br/><br/>


# Se√ß√£o 0Ô∏è‚É£: A Regra de Ouro

<br/>

## ‚ö™Ô∏è 0. A Regra de Ouro: Design para testes enxutos

:white_check_mark: **Fa√ßa:**
O c√≥digo de teste n√£o √© como o c√≥digo de produ√ß√£o - projete-o para ser simples, curto, sem abstra√ß√µes, plano, agrad√°vel de se trabalhar, enxuto. Deve-se olhar para um teste e obter a inten√ß√£o instantaneamente.

Nossas mentes est√£o cheias com o c√≥digo principal de produ√ß√£o, n√£o temos 'espa√ßo de sobra' para complexidade adicional. Se tentarmos espremer outro c√≥digo desafiador em nosso c√©rebro fraco, a equipe ficar√° mais lenta, o que vai de encontro com a raz√£o pela qual fazemos os testes. Praticamente √© aqui que muitas equipes abandonam os testes.

Os testes s√£o uma oportunidade para outra coisa - um assistente amig√°vel e sorridente, que √© agrad√°vel de trabalhar e oferece grande valor para um investimento t√£o pequeno. A ci√™ncia diz que temos dois sistemas cerebrais: o sistema 1, usado para atividades sem esfor√ßo, como dirigir um carro em uma estrada vazia, e o sistema 2, destinado a opera√ß√µes complexas e conscientes, como resolver uma equa√ß√£o matem√°tica. Projete seu teste para o sistema 1, ao analisar o c√≥digo de teste, ele deve parecer t√£o f√°cil quanto modificar um documento HTML e n√£o como resolver um equa√ß√£o 2X (17 √ó 24).

Isso pode ser alcan√ßado atrav√©s de t√©cnicas, ferramentas e alvos de teste selecionados de forma econ√¥mica, que s√£o econ√¥micos e proporcionam um √≥timo ROI. Teste apenas o necess√°rio, esforce-se para mant√™-lo √°gil, √†s vezes vale a pena abandonar alguns testes e trocar a confiabilidade por agilidade e simplicidade.

![alt text](/assets/headspace.png "We have no head room for additional complexity")
 
A maioria dos conselhos abaixo s√£o derivados desse princ√≠pio.

### Pronto para come√ßar?


<br/><br/>

# Se√ß√£o 1: A Anatomia do Teste

<br/>

## ‚ö™ Ô∏è 1.1 Inclua 3 partes em cada nome de teste

:white_check_mark: **Fa√ßa:** Um relat√≥rio de teste deve informar se a revis√£o atual do aplicativo atende aos requisitos para as pessoas que n√£o est√£o necessariamente familiarizadas com o c√≥digo: o testador, o engenheiro DevOps que est√° implantando e voc√™ daqui a dois anos. Isso pode ser melhor alcan√ßado se os testes falarem no n√≠vel de requisitos e incluirem 3 partes:

(1) O que est√° sendo testado? Por exemplo, o m√©todo ProductsService.addNewProduct

(2) Sob que circunst√¢ncias e cen√°rio? Por exemplo, nenhum pre√ßo √© passado para o m√©todo

(3) Qual √© o resultado esperado? Por exemplo, o novo produto n√£o √© aprovado

<br/>


‚ùå **Caso contr√°rio:** Uma implanta√ß√£o acabou de falhar, um teste chamado "Adicionar produto" falhou. Isso diz o que exatamente est√° com defeito?

<br/>

**üëá Nota:** Cada marcador possui exemplos de c√≥digo e alguns tem ilustra√ß√µes. Clique para expandir
<details><summary>‚úè <b>C√≥digos de Exemplo</b></summary>
  
<br/>
  
### :clap: Exemplo: um nome de teste que constitui 3 partes

![](https://img.shields.io/badge/üî®%20Example%20using%20Mocha-blue.svg
 "Using Mocha to illustrate the idea")

```javascript
//1. unit under test
describe('Products Service', function() {
  describe('Add new product', function() {
    //2. scenario and 3. expectation
    it('When no price is specified, then the product status is pending approval', ()=> {
      const newProduct = new ProductService().add(...);
      expect(newProduct.status).to.equal('pendingApproval');
    });
  });
});

```
<br/>

### :clap: Exemplo: um nome de teste que constitui 3 partes
![alt text](/assets/bp-1-3-parts.jpeg "A test name that constitutes 3 parts")

</details>

<br/><br/>

## ‚ö™ Ô∏è 1.2 Testes de estrutura pelo padr√£o em ingl√™s AAA

:white_check_mark: **Fa√ßa:** Estruture seus testes com 3 se√ß√µes bem separadas: Organizar, Atuar e Afirmar (OAA). Seguir essa estrutura garante que o leitor n√£o gaste CPU do c√©rebro na compreens√£o do plano de teste:

1st O - Organizar: todo o c√≥digo de configura√ß√£o para levar o sistema ao cen√°rio que o teste pretende simular. Isso pode incluir instanciar a unidade sob o construtor de teste, adicionar registros de banco de dados, mockar/stubbing de objetos e qualquer outro c√≥digo de prepara√ß√£o

2nd A - Ato: Execute teste em unidade. Geralmente 1 linha de c√≥digo

3rd A - Afirmar: Garanta que o valor recebido satisfa√ßa a expectativa. Geralmente 1 linha de c√≥digo


<br/>


‚ùå **Caso contr√°rio:** Voc√™ n√£o gata apenas longas horas di√°rias para entender o c√≥digo principal, agora tamb√©m o que deveria ter sido a parte simples do dia (teste) estica seu c√©rebro

<br/>

<details><summary>‚úè <b>C√≥digos de Exemplo</b></summary>

<br/>

### :clap: Doing It Right Example: A test structured with the AAA pattern

![](https://img.shields.io/badge/üîß%20Example%20using%20Jest-blue.svg
 "Examples with Jest") ![](https://img.shields.io/badge/üîß%20Example%20using%20Mocha-blue.svg
 "Examples with Jest")
  
```javascript
describe('Customer classifier', () => {
    test('When customer spent more than 500$, should be classified as premium', () => {
        //Arrange
        const customerToClassify = {spent:505, joined: new Date(), id:1}
        const DBStub = sinon.stub(dataAccess, "getCustomer")
            .reply({id:1, classification: 'regular'});

        //Act
        const receivedClassification = customerClassifier.classifyCustomer(customerToClassify);

        //Assert
        expect(receivedClassification).toMatch('premium');
    });
});
```

<br/>

### :thumbsdown: Anti Pattern Example: No separation, one bulk, harder to interpret

```javascript
test('Should be classified as premium', () => {
        const customerToClassify = {spent:505, joined: new Date(), id:1}
        const DBStub = sinon.stub(dataAccess, "getCustomer")
            .reply({id:1, classification: 'regular'});
        const receivedClassification = customerClassifier.classifyCustomer(customerToClassify);
        expect(receivedClassification).toMatch('premium');
    });
```


</details>



<br/><br/>




## ‚ö™ Ô∏è1.3 Describe expectations in a product language: use BDD-style assertions

:white_check_mark: **Do:** Coding your tests in a declarative-style allows the reader to get the grab instantly without spending even a single brain-CPU cycle. When you write an imperative code that is packed with conditional logic the reader is thrown away to an effortful mental mood. In that sense, code the expectation in a human-like language, declarative BDD style using expect or should and not using custom code. If Chai & Jest don‚Äôt include the desired assertion and it‚Äôs highly repeatable, consider [extending Jest matcher (Jest)](https://jestjs.io/docs/en/expect#expectextendmatchers) or writing a [custom Chai plugin](https://www.chaijs.com/guide/plugins/)
<br/>


‚ùå **Otherwise:** The team will write less test and decorate the annoying ones with .skip()

<br/>

<details><summary>‚úè <b>Code Examples</b></summary><br/>

![](https://img.shields.io/badge/üîß%20Example%20using%20Mocha-blue.svg
 "Examples with Mocha & Chai") ![](https://img.shields.io/badge/üîß%20Example%20using%20Jest-blue.svg
 "Examples with Jest")
  
  ### :thumbsdown: Anti Pattern Example: The reader must skim through not so short, and imperative code just to get the test story

```javascript
test("When asking for an admin, ensure only ordered admins in results" , () => {
    //assuming we've added here two admins "admin1", "admin2" and "user1"
    const allAdmins = getUsers({adminOnly:true});

    const admin1Found, adming2Found = false;

    allAdmins.forEach(aSingleUser => {
        if(aSingleUser === "user1"){
            assert.notEqual(aSingleUser, "user1", "A user was found and not admin");
        }
        if(aSingleUser==="admin1"){
            admin1Found = true;
        }
        if(aSingleUser==="admin2"){
            admin2Found = true;
        }
    });

    if(!admin1Found || !admin2Found ){
        throw new Error("Not all admins were returned");
    }
});

```
<br/>

### :clap: Doing It Right Example: Skimming through the following declarative test is a breeze


```javascript
it("When asking for an admin, ensure only ordered admins in results" , () => {
    //assuming we've added here two admins
    const allAdmins = getUsers({adminOnly:true});

    expect(allAdmins).to.include.ordered.members(["admin1" , "admin2"])
  .but.not.include.ordered.members(["user1"]);
});

```

</details>


<br/><br/>


## ‚ö™ Ô∏è  1.4 Stick to black-box testing: Test only public methods

:white_check_mark: **Do:** Testing the internals brings huge overhead for almost nothing. If your code/API deliver the right results, should you really invest your next 3 hours in testing HOW it worked internally and then maintain these fragile tests? Whenever a public behavior is checked, the private implementation is also implicitly tested and your tests will break only if there is a certain problem (e.g. wrong output). This approach is also referred to as behavioral testing. On the other side, should you test the internals (white box approach)‚Ää‚Äî‚Ääyour focus shifts from planning the component outcome to nitty-gritty details and your test might break because of minor code refactors although the results are fine- this dramatically increases the maintenance burden
<br/>


‚ùå **Otherwise:** Your test behaves like the [child who cries wolf](https://en.wikipedia.org/wiki/The_Boy_Who_Cried_Wolf): shoot out loud false-positive cries (e.g., A test fails because a private variable name was changed). Unsurprisingly, people will soon start to ignore the CI notifications until someday a real bug will get ignored‚Ä¶

<br/>
<details><summary>‚úè <b>Code Examples</b></summary>

<br/>

### :thumbsdown: Anti Pattern Example: A test case is testing the internals for no good reason
![](https://img.shields.io/badge/üîß%20Example%20using%20Mocha-blue.svg
 "Examples with Mocha & Chai")
```javascript
class ProductService{
  //this method is only used internally
  //Change this name will make the tests fail
  calculateVAT(priceWithoutVAT){
    return {finalPrice: priceWithoutVAT * 1.2};
    //Change the result format or key name above will make the tests fail
  }
  //public method
  getPrice(productId){
    const desiredProduct= DB.getProduct(productId);
    finalPrice = this.calculateVATAdd(desiredProduct.price).finalPrice;
  }
}


it("White-box test: When the internal methods get 0 vat, it return 0 response", async () => {
    //There's no requirement to allow users to calculate the VAT, only show the final price. Nevertheless we falsely insist here to test the class internals
    expect(new ProductService().calculateVATAdd(0).finalPrice).to.equal(0);
});

```

</details>




<br/><br/>

## ‚ö™ Ô∏è Ô∏è1.5 Choose the right test doubles: Avoid mocks in favor of stubs and spies

:white_check_mark: **Do:**  Test doubles are a necessary evil because they are coupled to the application internals, yet some provide an immense value (<a href="https://martinfowler.com/articles/mocksArentStubs.html" data-href="https://martinfowler.com/articles/mocksArentStubs.html" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">[Read here a reminder about test doubles: mocks vs stubs vs spies](https://martinfowler.com/articles/mocksArentStubs.html)</a>).

Before using test doubles, ask a very simple question: Do I use it to test functionality that appears, or could appear, in the requirements document? If no, it‚Äôs a smell of white-box testing.

For example, if you want to test what your app behaves reasonably when the payment service is down, you might stub the payment service and trigger some ‚ÄòNo Response‚Äô return to ensure that the unit under test returns the right value. This checks our application behavior/response/outcome under certain scenarios. You might also use a spy to assert that an email was sent when that service is down‚Ää‚Äî‚Ääthis is again a behavioral check which is likely to appear in a requirements doc (‚ÄúSend an email if payment couldn‚Äôt be saved‚Äù). On the flip side, if you mock the Payment service and ensure that it was called with the right JavaScript types‚Ää‚Äî‚Ääthen your test is focused on internal things that got nothing with the application functionality and are likely to change frequently
<br/>


‚ùå **Otherwise:** Any refactoring of code mandates searching for all the mocks in the code and updating accordingly. Tests become a burden rather than a helpful friend

<br/>

<details><summary>‚úè <b>Code Examples</b></summary>

<br/>

### :thumbsdown: Anti-pattern example: Mocks focus on the internals
![](https://img.shields.io/badge/üîß%20Example%20using%20Sinon-blue.svg
 "Examples with Mocha & Chai")
```javascript
it("When a valid product is about to be deleted, ensure data access DAL was called once, with the right product and right config", async () => {
    //Assume we already added a product
    const dataAccessMock = sinon.mock(DAL);
    //hmmm BAD: testing the internals is actually our main goal here, not just a side-effect
    dataAccessMock.expects("deleteProduct").once().withArgs(DBConfig, theProductWeJustAdded, true, false);
    new ProductService().deletePrice(theProductWeJustAdded);
    dataAccessMock.verify();
});
```
<br/>

### :clap:Doing It Right Example: spies are focused on testing the requirements but as a side-effect are unavoidably touching to the internals

```javascript
it("When a valid product is about to be deleted, ensure an email is sent", async () => {
    //Assume we already added here a product
    const spy = sinon.spy(Emailer.prototype, "sendEmail");
    new ProductService().deletePrice(theProductWeJustAdded);
    //hmmm OK: we deal with internals? Yes, but as a side effect of testing the requirements (sending an email)
});
```

</details>



<br/><br/>

## ‚ö™ Ô∏è1.6 Don‚Äôt ‚Äúfoo‚Äù, use realistic input data

:white_check_mark: **Do:**  Often production bugs are revealed under some very specific and surprising input‚Ää‚Äî‚Ääthe more realistic the test input is, the greater the chances are to catch bugs early. Use dedicated libraries like [Faker](https://www.npmjs.com/package/faker) to generate pseudo-real data that resembles the variety and form of production data. For example, such libraries can generate realistic phone numbers, usernames, credit card, company names, and even ‚Äòlorem ipsum‚Äô text. You may also create some tests (on top of unit tests, not instead) that randomize fakers data to stretch your unit under test or even import real data from your production environment. Want to take it to the next level? see next bullet (property-based testing).
<br/>


‚ùå **Otherwise:** All your development testing will falsely seem green when you use synthetic inputs like ‚ÄúFoo‚Äù but then production might turn red when a hacker passes-in a nasty string like ‚Äú@3e2ddsf . ##‚Äô 1 fdsfds . fds432 AAAA‚Äù


<br/>

<details><summary>‚úè <b>Code Examples</b></summary>

<br/>

### :thumbsdown: Anti-Pattern Example: A test suite that passes due to non-realistic data

![](https://img.shields.io/badge/üîß%20Example%20using%20Jest-blue.svg
 "Examples with Jest")
 
 
```javascript
const addProduct = (name, price) =>{
  const productNameRegexNoSpace = /^\S*$/;//no white-space allowd

  if(!productNameRegexNoSpace.test(name))
    return false;//this path never reached due to dull input

    //some logic here
    return true;
};

test("Wrong: When adding new product with valid properties, get successful confirmation", async () => {
    //The string "Foo" which is used in all tests never triggers a false result
    const addProductResult = addProduct("Foo", 5);
    expect(addProductResult).toBe(true);
    //Positive-false: the operation succeeded because we never tried with long
    //product name including spaces
});

```
<br/>

### :clap:Doing It Right Example: Randomizing realistic input
```javascript
it("Better: When adding new valid product, get successful confirmation", async () => {
    const addProductResult = addProduct(faker.commerce.productName(), faker.random.number());
    //Generated random input: {'Sleek Cotton Computer',  85481}
    expect(addProductResult).to.be.true;
    //Test failed, the random input triggered some path we never planned for.
    //We discovered a bug early!
});
```

</details>




<br/><br/>

## ‚ö™ Ô∏è 1.7 Test many input combinations using Property-based testing

:white_check_mark: **Do:** Typically we choose a few input samples for each test. Even when the input format resembles real-world data (see bullet ‚ÄòDon‚Äôt foo‚Äô), we cover only a few input combinations (method(‚Äò‚Äô, true, 1), method(‚Äústring‚Äù , false‚Äù , 0)), However, in production, an API that is called with 5 parameters can be invoked with thousands of different permutations, one of them might render our process down ([see Fuzz Testing](https://en.wikipedia.org/wiki/Fuzzing)). What if you could write a single test that sends 1000 permutations of different inputs automatically and catches for which input our code fails to return the right response? Property-based testing is a technique that does exactly that: by sending all the possible input combinations to your unit under test it increases the serendipity of finding a bug. For example, given a method‚Ää‚Äî‚ÄäaddNewProduct(id, name, isDiscount)‚Ää‚Äî‚Ääthe supporting libraries will call this method with many combinations of (number, string, boolean) like (1, ‚ÄúiPhone‚Äù, false), (2, ‚ÄúGalaxy‚Äù, true). You can run property-based testing using your favorite test runner (Mocha, Jest, etc) using libraries like [js-verify](https://github.com/jsverify/jsverify) or [testcheck](https://github.com/leebyron/testcheck-js) (much better documentation). Update: Nicolas Dubien suggests in the comments below to [checkout fast-check](https://github.com/dubzzz/fast-check#readme) which seems to offer some additional features and also to be actively maintained
<br/>


‚ùå **Otherwise:** Unconsciously, you choose the test inputs that cover only code paths that work well. Unfortunately, this decreases the efficiency of testing as a vehicle to expose bugs


<br/>

<details><summary>‚úè <b>Code Examples</b></summary>

<br/>

### :clap:  Doing It Right Example: Testing many input permutations with ‚Äúmocha-testcheck‚Äù

![](https://img.shields.io/badge/üîß%20Example%20using%20Mocha-blue.svg
 "Examples with Jest")

```javascript
require('mocha-testcheck').install();
const {expect} = require('chai');

describe('Product service', () => {
  describe('Adding new', () => {
    //this will run 100 times with different random properties
    check.it('Add new product with random yet valid properties, always successful',
      gen.int, gen.string, (id, name) => {
        expect(addNewProduct(id, name).status).to.equal('approved');
      });
  })
});

```

</details>




<br/><br/>

## ‚ö™ Ô∏è 1.8 If needed, use only short & inline snapshots

:white_check_mark: **Do:** When there is a need for [snapshot testing](https://jestjs.io/docs/en/snapshot-testing), use only short and focused snapshots (i.e. 3-7 lines) that are included as part of the test ([Inline Snapshot](https://jestjs.io/docs/en/snapshot-testing#inline-snapshots)) and not within external files. Keeping this guideline will ensure your tests remain self-explanatory and less fragile.

On the other hand, ‚Äòclassic snapshots‚Äô tutorials and tools encourage to store big files (e.g. component rendering markup, API JSON result) over some external medium and ensure each time when the test run to compare the received result with the saved version. This, for example, can implicitly couple our test to 1000 lines with 3000 data values that the test writer never read and reasoned about. Why is this wrong? By doing so, there are 1000 reasons for your test to fail - it‚Äôs enough for a single line to change for the snapshot to get invalid and this is likely to happen a lot. How frequently? for every space, comment or minor CSS/HTML change. Not only this, the test name wouldn‚Äôt give a clue about the failure as it just checks that 1000 lines didn‚Äôt change, also it encourages to the test writer to accept as the desired true a long document he couldn‚Äôt inspect and verify. All of these are symptoms of obscure and eager test that is not focused and aims to achieve too much

It‚Äôs worth noting that there are few cases where long & external snapshots are acceptable - when asserting on schema and not data (extracting out values and focusing on fields) or when the received document rarely changes
<br/>

‚ùå **Otherwise:** A UI test fails. The code seems right, the screen renders perfect pixels, what happened? your snapshot testing just found a difference from the origin document to current received one - a single space character was added to the markdown... 

<br/>

<details><summary>‚úè <b>Code Examples</b></summary>

<br/>

### :thumbsdown: Anti-Pattern Example: Coupling our test to unseen 2000 lines of code

![](https://img.shields.io/badge/üîß%20Example%20using%20Jest-blue.svg
 "Examples with Jest")
 
```javascript
it('TestJavaScript.com is renderd correctly', ()  => {

//Arrange

//Act
const receivedPage = renderer
.create(  <DisplayPage page  =  "http://www.testjavascript.com"  > Test JavaScript < /DisplayPage>)
.toJSON();

//Assert
expect(receivedPage).toMatchSnapshot();
//We now implicitly maintain a 2000 lines long document
//every additional line break or comment - will break this test

});
```
<br/>

### :clap: Doing It Right Example: Expectations are visible and focused
```javascript
it('When visiting TestJavaScript.com home page, a menu is displayed', () => {
//Arrange

//Act
receivedPage tree = renderer
.create(  <DisplayPage page  =  "http://www.testjavascript.com"  > Test JavaScript < /DisplayPage>)
.toJSON();

//Assert

const menu = receivedPage.content.menu;
expect(menu).toMatchInlineSnapshot(`
<ul>
<li>Home</li>
<li> About </li>
<li> Contact </li>
</ul>
`);
});
```

</details>


<br/><br/>

## ‚ö™ Ô∏è1.9 Avoid global test fixtures and seeds, add data per-test

:white_check_mark: **Do:** Going by the golden rule (bullet 0), each test should add and act on its own set of DB rows to prevent coupling and easily reason about the test flow. In reality, this is often violated by testers who seed the DB with data before running the tests ([also known as ‚Äòtest fixture‚Äô](https://en.wikipedia.org/wiki/Test_fixture)) for the sake of performance improvement. While performance is indeed a valid concern‚Ää‚Äî‚Ääit can be mitigated (see ‚ÄúComponent testing‚Äù bullet), however, test complexity is a much painful sorrow that should govern other considerations most of the time. Practically, make each test case explicitly add the DB records it needs and act only on those records. If performance becomes a critical concern‚Ää‚Äî‚Ääa balanced compromise might come in the form of seeding the only suite of tests that are not mutating data (e.g. queries)
<br/>


‚ùå **Otherwise:** Few tests fail, a deployment is aborted, our team is going to spend precious time now, do we have a bug? let‚Äôs investigate, oh no‚Ää‚Äî‚Ääit seems that two tests were mutating the same seed data


<br/>

<details><summary>‚úè <b>Code Examples</b></summary>

<br/>

### :thumbsdown: Anti Pattern Example: tests are not independent and rely on some global hook to feed global DB data

![](https://img.shields.io/badge/üîß%20Example%20using%20Mocha-blue.svg
 "Examples with Jest")
 
```javascript
before(() => {
  //adding sites and admins data to our DB. Where is the data? outside. At some external json or migration framework
  await DB.AddSeedDataFromJson('seed.json');
});
it("When updating site name, get successful confirmation", async () => {
  //I know that site name "portal" exists - I saw it in the seed files
  const siteToUpdate = await SiteService.getSiteByName("Portal");
  const updateNameResult = await SiteService.changeName(siteToUpdate, "newName");
  expect(updateNameResult).to.be(true);
});
it("When querying by site name, get the right site", async () => {
  //I know that site name "portal" exists - I saw it in the seed files
  const siteToCheck = await SiteService.getSiteByName("Portal");
  expect(siteToCheck.name).to.be.equal("Portal"); //Failure! The previous test change the name :[
});

```
<br/>

### :clap: Doing It Right Example: We can stay within the test, each test acts on its own set of data

```javascript
it("When updating site name, get successful confirmation", async () => {
  //test is adding a fresh new records and acting on the records only
  const siteUnderTest = await SiteService.addSite({
    name: "siteForUpdateTest"
  });
  
  const updateNameResult = await SiteService.changeName(siteUnderTest, "newName");
  
  expect(updateNameResult).to.be(true);
});

```

</details>


<br/>

## ‚ö™ Ô∏è 1.10 Don‚Äôt catch errors, expect them
:white_check_mark: **Do:**   When trying to assert that some input triggers an error, it might look right to use try-catch-finally and asserts that the catch clause was entered. The result is an awkward and verbose test case (example below) that hides the simple test intent and the result expectations

A more elegant alternative is the using the one-line dedicated Chai assertion: expect(method).to.throw (or in Jest: expect(method).toThrow()). It‚Äôs absolutely mandatory to also ensure the exception contains a property that tells the error type, otherwise given just a generic error the application won‚Äôt be able to do much rather than show a disappointing message to the user
<br/>


‚ùå **Otherwise:** It will be challenging to infer from the test reports (e.g. CI reports) what went wrong


<br/>

<details><summary>‚úè <b>Code Examples</b></summary>

<br/>

### :thumbsdown: Anti-pattern Example: A long test case that tries to assert the existence of error with try-catch

![](https://img.shields.io/badge/üîß%20Example%20using%20Mocha-blue.svg
 "Examples with Jest")
 
```javascript
it("When no product name, it throws error 400", async() => {
let errorWeExceptFor = null;
try {
  const result = await addNewProduct({name:'nest'});}
catch (error) {
  expect(error.code).to.equal('InvalidInput');
  errorWeExceptFor = error;
}
expect(errorWeExceptFor).not.to.be.null;
//if this assertion fails, the tests results/reports will only show
//that some value is null, there won't be a word about a missing Exception
});

```
<br/>

### :clap: Doing It Right Example: A human-readable expectation that could be understood easily, maybe even by QA or technical PM

```javascript
it.only("When no product name, it throws error 400", async() => {
  expect(addNewProduct)).to.eventually.throw(AppError).with.property('code', "InvalidInput");
});

```

</details>




<br/><br/>

## ‚ö™ Ô∏è 1.11 Tag your tests

:white_check_mark: **Do:**  Different tests must run on different scenarios: quick smoke, IO-less, tests should run when a developer saves or commits a file, full end-to-end tests usually run when a new pull request is submitted, etc. This can be achieved by tagging tests with keywords like #cold #api #sanity so you can grep with your testing harness and invoke the desired subset. For example, this is how you would invoke only the sanity test group with Mocha: mocha‚Ää‚Äî‚Äägrep ‚Äòsanity‚Äô
<br/>


‚ùå **Otherwise:** Running all the tests, including tests that perform dozens of DB queries, any time a developer makes a small change can be extremely slow and keeps developers away from running tests


<br/>

<details><summary>‚úè <b>Code Examples</b></summary>

<br/>

### :clap: Doing It Right Example: Tagging tests as ‚Äò#cold-test‚Äô allows the test runner to execute only fast tests (Cold===quick tests that are doing no IO and can be executed frequently even as the developer is typing)

![](https://img.shields.io/badge/üîß%20Example%20using%20Jest-blue.svg
 "Examples with Jest")
```javascript
//this test is fast (no DB) and we're tagging it correspondigly
//now the user/CI can run it frequently
describe('Order service', function() {
  describe('Add new order #cold-test #sanity', function() {
    test('Scenario - no currency was supplied. Expectation - Use the default currency #sanity', function() {
      //code logic here
    });
  });
});


```

</details>




<br/><br/>

## ‚ö™ Ô∏è1.12 Other generic good testing hygiene
:white_check_mark: **Do:**  This post is focused on testing advice that is related to, or at least can be exemplified with Node JS. This bullet, however, groups few non-Node related tips that are well-known

Learn and practice [TDD principles](https://www.sm-cloud.com/book-review-test-driven-development-by-example-a-tldr/)‚Ää‚Äî‚Ääthey are extremely valuable for many but don‚Äôt get intimidated if they don‚Äôt fit your style, you‚Äôre not the only one. Consider writing the tests before the code in a [red-green-refactor style](https://blog.cleancoder.com/uncle-bob/2014/12/17/TheCyclesOfTDD.html), ensure each test checks exactly one thing, when you find a bug‚Ää‚Äî‚Ääbefore fixing write a test that will detect this bug in the future, let each test fail at least once before turning green, start a module by writing a quick and simplistic code that satsifies the test - then refactor gradually and take it to a production grade level, avoid any dependency on the environment (paths, OS, etc)
<br/>


‚ùå **Otherwise:** You‚Äòll miss pearls of wisdom that were collected for decades

<br/><br/>


# Section 2Ô∏è‚É£: Backend Testing

## ‚ö™ Ô∏è2.1 Enrich your testing portfolio: Look beyond unit tests and the pyramid

:white_check_mark: **Do:**  The [testing pyramid](https://martinfowler.com/bliki/TestPyramid.html), though 10> years old, is a great and relevant model that suggests three testing types and influences most developers‚Äô testing strategy. At the same time, more than a handful of shiny new testing techniques emerged and are hiding in the shadows of the testing pyramid. Given all the dramatic changes that we‚Äôve seen in the recent 10 years (Microservices, cloud, serverless), is it even possible that one quite-old model will suit *all* types of applications? shouldn‚Äôt the testing world consider welcoming new testing techniques?

Don‚Äôt get me wrong, in 2019 the testing pyramid, TDD and unit tests are still a powerful technique and are probably the best match for many applications. Only like any other model, despite its usefulness, [it must be wrong sometimes](https://en.wikipedia.org/wiki/All_models_are_wrong). For example, consider an IOT application that ingests many events into a message-bus like Kafka/RabbitMQ, which then flow into some data-warehouse and are eventually queried by some analytics UI. Should we really spend 50% of our testing budget on writing unit tests for an application that is integration-centric and has almost no logic? As the diversity of application types increase (bots, crypto, Alexa-skills) greater are the chances to find scenarios where the testing pyramid is not the best match.

It‚Äôs time to enrich your testing portfolio and become familiar with more testing types (the next bullets suggest few ideas), mind models like the testing pyramid but also match testing types to real-world problems that you‚Äôre facing (‚ÄòHey, our API is broken, let‚Äôs write consumer-driven contract testing!‚Äô), diversify your tests like an investor that build a portfolio based on risk analysis‚Ää‚Äî‚Ääassess where problems might arise and match some prevention measures to mitigate those potential risks

A word of caution: the TDD argument in the software world takes a typical false-dichotomy face, some preach to use it everywhere, others think it‚Äôs the devil. Everyone who speaks in absolutes is wrong :]

<br/>


‚ùå **Otherwise:** You‚Äôre going to miss some tools with amazing ROI, some like Fuzz, lint, and mutation can provide value in 10 minutes


<br/>

<details><summary>‚úè <b>Code Examples</b></summary>

<br/>

### :clap: Doing It Right Example: Cindy Sridharan suggests a rich testing portfolio in her amazing post ‚ÄòTesting Microservices‚Ää‚Äî‚Ääthe sane way‚Äô
![alt text](assets/bp-12-rich-testing.jpeg "Cindy Sridharan suggests a rich testing portfolio in her amazing post ‚ÄòTesting Microservices‚Ää‚Äî‚Ääthe sane way‚Äô")

<strong class="markup--strong markup--p-strong">‚ò∫Ô∏èExample: </strong><a href="https://www.youtube.com/watch?v=-2zP494wdUY&amp;feature=youtube" data-href="https://www.youtube.com/watch?v=-2zP494wdUY&amp;feature=youtu.be" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">[YouTube: ‚ÄúBeyond Unit Tests: 5 Shiny Node.JS Test Types (2018)‚Äù (Yoni Goldberg)](https://www.youtube.com/watch?v=-2zP494wdUY&feature=youtu.be)</a>

<br/>

![alt text](assets/bp-12-Yoni-Goldberg-Testing.jpeg "A test name that constitutes 3 parts")


</details>




<br/><br/>

## ‚ö™ Ô∏è2.2 Component testing might be your best affair

:white_check_mark: **Do:** Each unit test covers a tiny portion of the application and it‚Äôs expensive to cover the whole, whereas end-to-end testing easily covers a lot of ground but is flaky and slower, why not apply a balanced approach and write tests that are bigger than unit tests but smaller than end-to-end testing? Component testing is the unsung song of the testing world‚Ää‚Äî‚Ääthey provide the best from both worlds: reasonable performance and a possibility to apply TDD patterns + realistic and great coverage.

Component tests focus on the Microservice ‚Äòunit‚Äô, they work against the API, don‚Äôt mock anything which belongs to the Microservice itself (e.g. real DB, or at least the in-memory version of that DB) but stub anything that is external like calls to other Microservices. By doing so, we test what we deploy, approach the app from outwards to inwards and gain great confidence in a reasonable amount of time.
<br/>


‚ùå **Otherwise:** You may spend long days on writing unit tests to find out that you got only 20% system coverage


<br/>

<details><summary>‚úè <b>Code Examples</b></summary>

<br/>

### :clap: Doing It Right Example: Supertest allows approaching Express API in-process (fast and cover many layers)

![](https://img.shields.io/badge/üîß%20Example%20using%20Mocha-blue.svg
 "Examples with Jest")

![alt text](assets/bp-13-component-test-yoni-goldberg.png " [Supertest](https://www.npmjs.com/package/supertest) allows approaching Express API in-process (fast and cover many layers)")

</details>

<br/><br/>

## ‚ö™ Ô∏è2.3 Ensure new releases don‚Äôt break the API using

:white_check_mark: **Do:**  So your Microservice has multiple clients, and you run multiple versions of the service for compatibility reasons (keeping everyone happy). Then you change some field and ‚Äòboom!‚Äô, some important client who relies on this field is angry. This is the Catch-22 of the integration world: It‚Äôs very challenging for the server side to consider all the multiple client expectations‚Ää‚Äî‚ÄäOn the other hand, the clients can‚Äôt perform any testing because the server controls the release dates. [Consumer-driven contracts and the framework PACT](https://docs.pact.io/) were born to formalize this process with a very disruptive approach‚Ää‚Äî‚Äänot the server defines the test plan of itself rather the client defines the tests of the‚Ä¶ server! PACT can record the client expectation and put in a shared location, ‚Äúbroker‚Äù, so the server can pull the expectations and run on every build using PACT library to detect broken contracts‚Ää‚Äî‚Ääa client expectation that is not met. By doing so, all the server-client API mismatches are caught early during build/CI and might save you a great deal of frustration
<br/>


‚ùå **Otherwise:** The alternatives are exhausting manual testing or deployment fear


<br/>

<details><summary>‚úè <b>Code Examples</b></summary>

<br/>

### :clap: Doing It Right Example:

![](https://img.shields.io/badge/üîß%20Example%20using%20PACT-blue.svg
 "Examples with PACT")
 
![alt text](assets/bp-14-testing-best-practices-contract-flow.png )


</details>



<br/><br/>


## ‚ö™ Ô∏è 2.4 Test your middlewares in isolation

:white_check_mark: **Do:** Many avoid Middleware testing because they represent a small portion of the system and require a live Express server. Both reasons are wrong‚Ää‚Äî‚ÄäMiddlewares are small but affect all or most of the requests and can be tested easily as pure functions that get {req,res} JS objects. To test a middleware function one should just invoke it and spy ([using Sinon for example](https://www.npmjs.com/package/sinon)) on the interaction with the {req,res} objects to ensure the function performed the right action. The library [node-mock-http](https://www.npmjs.com/package/node-mocks-http) takes it even further and factors the {req,res} objects along with spying on their behavior. For example, it can assert whether the http status that was set on the res object matches the expectation (See example below)
<br/>


‚ùå **Otherwise:** A bug in Express middleware === a bug in all or most requests


<br/>

<details><summary>‚úè <b>Code Examples</b></summary>

<br/>

### :clap:Doing It Right Example: Testing middleware in isolation without issuing network calls and waking-up the entire Express machine

![](https://img.shields.io/badge/üîß%20Example%20using%20Jest-blue.svg
 "Examples with Jest")

```javascript
//the middleware we want to test
const unitUnderTest = require('./middleware')
const httpMocks = require('node-mocks-http');
//Jest syntax, equivelant to describe() & it() in Mocha
test('A request without authentication header, should return http status 403', () => {
  const request = httpMocks.createRequest({
    method: 'GET',
    url: '/user/42',
    headers: {
      authentication: ''
    }
  });
  const response = httpMocks.createResponse();
  unitUnderTest(request, response);
  expect(response.statusCode).toBe(403);
});

```

</details>




<br/><br/>

## ‚ö™ Ô∏è2.5 Measure and refactor using static analysis tools
:white_check_mark: **Do:** Using static analysis tools helps by giving objective ways to improve code quality and keep your code maintainable. You can add static analysis tools to your CI build to abort when it finds code smells. Its main selling points over plain linting are the ability to inspect quality in the context of multiple files (e.g. detect duplications), perform advanced analysis (e.g. code complexity) and follow the history and progress of code issues. Two examples of tools you can use are [Sonarqube](https://www.sonarqube.org/) (2,600+ [stars](https://github.com/SonarSource/sonarqube)) and [Code Climate](https://codeclimate.com/) (1,500+ [stars](https://github.com/codeclimate/codeclimate))

Credit:: <a href="https://github.com/TheHollidayInn" data-href="https://github.com/TheHollidayInn" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">[Keith Holliday](https://github.com/TheHollidayInn)</a>

<br/>


‚ùå **Otherwise:** With poor code quality, bugs and performance will always be an issue that no shiny new library or state of the art features can fix


<br/>

<details><summary>‚úè <b>Code Examples</b></summary>

<br/>

### :clap: Doing It Right Example:  CodeClimate, a commercial tool that can identify complex methods:

![](https://img.shields.io/badge/üîß%20Example%20using%20Code%20Climate-blue.svg
 "Examples with CodeClimate")
 
![alt text](assets/bp-16-yoni-goldberg-quality.png " CodeClimat, a commercial tool that can identify complex methods:")

</details>




<br/><br/>

## ‚ö™ Ô∏è 2.6 Check your readiness for Node-related chaos
:white_check_mark: **Do:** Weirdly, most software testings are about logic & data only, but some of the worst things that happen (and are really hard to mitigate ) are infrastructural issues. For example, did you ever test what happens when your process memory is overloaded, or when the server/process dies, or does your monitoring system realizes when the API becomes 50% slower?. To test and mitigate these type of bad things‚Ää‚Äî‚Ää[Chaos engineering](https://principlesofchaos.org/) was born by Netflix. It aims to provide awareness, frameworks and tools for testing our app resiliency for chaotic issues. For example, one of its famous tools, [the chaos monkey](https://github.com/Netflix/chaosmonkey), randomly kills servers to ensure that our service can still serve users and not relying on a single server (there is also a Kubernetes version, [kube-monkey](https://github.com/asobti/kube-monkey), that kills pods). All these tools work on the hosting/platform level, but what if you wish to test and generate pure Node chaos like check how your Node process copes with uncaught errors, unhandled promise rejection, v8 memory overloaded with the max allowed of 1.7GB or whether your UX stays satisfactory when the event loop gets blocked often? to address this I‚Äôve written, [node-chaos](https://github.com/i0natan/node-chaos-monkey) (alpha) which provides all sort of Node-related chaotic acts
<br/>


‚ùå **Otherwise:**  No escape here, Murphy‚Äôs law will hit your production without mercy


<br/>

<details><summary>‚úè <b>Code Examples</b></summary>

<br/>

### :clap: Doing It Right Example: : Node-chaos can generate all sort of Node.js pranks so you can test how resilience is your app to chaos
![alt text](assets/bp-17-yoni-goldberg-chaos-monkey-nodejs.png "Node-chaos can generate all sort of Node.js pranks so you can test how resilience is your app to chaos")

</details>

<br/>

## ‚ö™ Ô∏è2.7 Avoid global test fixtures and seeds, add data per-test

:white_check_mark: **Do:** Going by the golden rule (bullet 0), each test should add and act on its own set of DB rows to prevent coupling and easily reason about the test flow. In reality, this is often violated by testers who seed the DB with data before running the tests (also known as ‚Äòtest fixture‚Äô) for the sake of performance improvement. While performance is indeed a valid concern‚Ää‚Äî‚Ääit can be mitigated (see ‚ÄúComponent testing‚Äù bullet), however, test complexity is a much painful sorrow that should govern other considerations most of the time. Practically, make each test case explicitly add the DB records it needs and act only on those records. If performance becomes a critical concern‚Ää‚Äî‚Ääa balanced compromise might come in the form of seeding the only suite of tests that are not mutating data (e.g. queries)
<br/>


‚ùå **Otherwise:** Few tests fail, a deployment is aborted, our team is going to spend precious time now, do we have a bug? let‚Äôs investigate, oh no‚Ää‚Äî‚Ääit seems that two tests were mutating the same seed data


<br/>

<details><summary>‚úè <b>Code Examples</b></summary>

<br/>

### :thumbsdown: Anti Pattern Example: tests are not independent and rely on some global hook to feed global DB data

![](https://img.shields.io/badge/üîß%20Example%20using%20Mocha-blue.svg
 "Examples with Mocha")
 
```javascript
before(() => {
  //adding sites and admins data to our DB. Where is the data? outside. At some external json or migration framework
  await DB.AddSeedDataFromJson('seed.json');
});
it("When updating site name, get successful confirmation", async () => {
  //I know that site name "portal" exists - I saw it in the seed files
  const siteToUpdate = await SiteService.getSiteByName("Portal");
  const updateNameResult = await SiteService.changeName(siteToUpdate, "newName");
  expect(updateNameResult).to.be(true);
});
it("When querying by site name, get the right site", async () => {
  //I know that site name "portal" exists - I saw it in the seed files
  const siteToCheck = await SiteService.getSiteByName("Portal");
  expect(siteToCheck.name).to.be.equal("Portal"); //Failure! The previous test change the name :[
});

```
<br/>

### :clap: Doing It Right Example: We can stay within the test, each test acts on its own set of data

```javascript
it("When updating site name, get successful confirmation", async () => {
  //test is adding a fresh new records and acting on the records only
  const siteUnderTest = await SiteService.addSite({
    name: "siteForUpdateTest"
  });
  const updateNameResult = await SiteService.changeName(siteUnderTest, "newName");
  expect(updateNameResult).to.be(true);
});

```

</details>

<br/><br/>

# Se√ß√£o 3Ô∏è‚É£: Teste de Frontend

## ‚ö™ Ô∏è 3.1. Separar UI da funcionalidade

:white_check_mark: **Fa√ßa:** Ao focar no teste da l√≥gica dos componentes, os detalhes da interface do usu√°rio se tornam um ru√≠do que deve ser extra√≠do, para que seus testes possam se concentrar em dados puros. Na pr√°tica, extraia os dados desejados da marca√ß√£o de uma maneira abstrata que n√£o seja muito acoplada √† implementa√ß√£o gr√°fica, afirme apenas dados puros (vs detalhes gr√°ficos de HTML/CSS) e desative anima√ß√µes que diminuem a velocidade. Voc√™ pode cair na tenta√ß√£o de evitar renderizar e testar apenas a parte de tr√°s da interface do usu√°rio (por exemplo, servi√ßos, a√ß√µes, armazenamento), mas isso resultar√° em testes fict√≠cios que n√£o se assemelham √† realidade e n√£o revelam casos em que os dados corretos nem chegam na interface do usu√°rio


<br/>

‚ùå **Caso contr√°rio:** Os dados puramente calculados do seu teste podem estar prontos em 10 ms, mas o teste inteiro durar√° 500 ms (100 testes = 1 min) devido a alguma anima√ß√£o sofisticada e irrelevante


<br/>

<details><summary>‚úè <b>C√≥digos de Exemplo</b></summary>

<br/>

### :clap: Exemplo Fazendo Certo: Separando os detalhes da interface do usu√°rio

![](https://img.shields.io/badge/üîß%20Exemplo%20usando%20React-blue.svg
 "Exemplos com React") ![](https://img.shields.io/badge/üîß%20Exemplo%20usando%20React%20Testing%20Library-blue.svg
 "Exemplos com react-testing-library")

```javascript
test('When users-list is flagged to show only VIP, should display only VIP members', () => {
  // Arrange
  const allUsers = [
   { id: 1, name: 'Yoni Goldberg', vip: false }, 
   { id: 2, name: 'John Doe', vip: true }
  ];

  // Act
  const { getAllByTestId } = render(<UsersList users={allUsers} showOnlyVIP={true}/>);

  // Assert - Extract the data from the UI first
  const allRenderedUsers = getAllByTestId('user').map(uiElement => uiElement.textContent);
  const allRealVIPUsers = allUsers.filter((user) => user.vip).map((user) => user.name);
  expect(allRenderedUsers).toEqual(allRealVIPUsers); //compare data with data, no UI here
});

```

<br/>

### :thumbsdown: Exemplo Anti-padr√£o: Asser√ß√µes misturam detalhes da UI e dados
```javascript
test('When flagging to show only VIP, should display only VIP members', () => {
  // Arrange
  const allUsers = [
   {id: 1, name: 'Yoni Goldberg', vip: false }, 
   {id: 2, name: 'John Doe', vip: true }
  ];

  // Act
  const { getAllByTestId } = render(<UsersList users={allUsers} showOnlyVIP={true}/>);

  // Assert - Mix UI & data in assertion
  expect(getAllByTestId('user')).toEqual('[<li data-testid="user">John Doe</li>]');
});

```

</details>




<br/><br/>


## ‚ö™ Ô∏è 3.2 Consultar elementos HTML com base em atributos que provavelmente n√£o ser√£o alterados

:white_check_mark: **Fa√ßa*:** Consulte elementos HTML com base em atributos que provavelmente sobreviver√£o a altera√ß√µes gr√°ficas, diferentemente dos seletores CSS e sim como os r√≥tulos de formul√°rio. Se o elemento designado n√£o tiver esses atributos, crie um atributo dedicado a teste  como 'test-id-submit-button'. Seguir essa rota n√£o apenas garante que seus testes funcionais/l√≥gicos nunca sejam quebrados devido a altera√ß√µes de apar√™ncia, mas tamb√©m fica claro para toda a equipe que esse elemento e atributo s√£o utilizados por testes e n√£o devem ser removidos

<br/>

‚ùå **Caso contr√°rio:** Voc√™ deseja testar a funcionalidade de login que abrange muitos componentes, l√≥gica e servi√ßos, tudo est√° configurado perfeitamente - stubs, spies, chamadas Ajax s√£o isoladas. Tudo parece perfeito. Em seguida, o teste falha porque o designer alterou a classe CSS da div de 'thick-border' para 'thin-border'

<br/>

<details><summary>‚úè <b>C√≥digos de Exemplo</b></summary>

<br/>

### :clap: Exemplo Fazendo Certo: Consultando um elemento usando um atributo dedicado para teste

![](https://img.shields.io/badge/üîß%20Exemplo%20usando%20React-blue.svg
 "Exemplos com React")
 
```html
// the markup code (part of React component)
<h3>
  <Badge pill className="fixed_badge" variant="dark">
    <span data-testid="errorsLabel">{value}</span> <!-- note the attribute data-testid -->
  </Badge>
</h3>
```

```javascript
// this example is using react-testing-library
  test('Whenever no data is passed to metric, show 0 as default', () => {
    // Arrange
    const metricValue = undefined;

    // Act
    const { getByTestId } = render(<dashboardMetric value={undefined}/>);    
    
    expect(getByTestId('errorsLabel')).text()).toBe("0");
  });

```

<br/>

### :thumbsdown: Exemplo Anti-padr√£o: Confiando em atributos CSS
```html
<!-- the markup code (part of React component) -->
<span id="metric" className="d-flex-column">{value}</span> <!-- what if the designer changes the classs? -->
```

```javascript
// this exammple is using enzyme
test('Whenever no data is passed, error metric shows zero', () => {
    // ...
    
    expect(wrapper.find("[className='d-flex-column']").text()).toBe("0");
  });
```


</details>




<br/>

## ‚ö™ Ô∏è 3.3 Sempre que poss√≠vel, teste com um componente realista e totalmente renderizado

:white_check_mark: **Fa√ßa:** Sempre que tiver um tamanho razo√°vel, teste seu componente de fora como os usu√°rios, renderize a interface do usu√°rio, atue sobre ela e afirme que a interface do usu√°rio renderizada se comporta conforme o esperado. Evite todo tipo de simula√ß√£o, renderiza√ß√£o parcial e superficial - essa abordagem pode resultar em erros n√£o capturados devido √† falta de detalhes e dificultar a manuten√ß√£o, pois os testes interferem nos internos (veja o marcador 'Favorecer o teste de caixa preta'). Se um dos componentes filhos estiver desacelerando significativamente (por exemplo, anima√ß√£o) ou complicando a instala√ß√£o - considere substitu√≠-lo explicitamente por um falso

Com tudo isso dito, uma palavra de cautela √© necess√°ria: essa t√©cnica funciona para componentes pequenos/m√©dios que cont√™m um tamanho razo√°vel de componentes filhos. A renderiza√ß√£o completa de um componente com muitos filhos dificultar√° o racioc√≠nio sobre falhas de teste (an√°lise de causa raiz) e poder√° ficar muito lenta. Nesses casos, escreva apenas alguns testes contra esse componente pai pesado e mais testes contra seus filhos

<br/>

‚ùå **Caso contr√°rio:** Ao entrar no interno de um componente, invocando seus m√©todos privados e verificando o estado interno - voc√™ teria que refatorar todos os testes ao refatorar a implementa√ß√£o dos componentes. Voc√™ realmente tem capacidade para esse n√≠vel de manuten√ß√£o?


<br/>

<details><summary>‚úè <b>C√≥digos de Exemplo</b></summary>

<br/>

### :clap: Exemplo Fazendo Certo: Trabalhando realisticamente com um componente totalmente renderizado

![](https://img.shields.io/badge/üîß%20Exemplo%20usando%20React-blue.svg
 "Exemplos com React") ![](https://img.shields.io/badge/üîß%20Exemplo%20usando%20Enzyme-blue.svg
 "Exemplos com Enzyme")
 
```javascript
class Calendar extends React.Component {
  static defaultProps = {showFilters: false}
  
  render() {
    return (
      <div>
        A filters panel with a button to hide/show filters
        <FiltersPanel showFilter={showFilters} title='Choose Filters'/>
      </div>
    )
  }
}

//Examples use React & Enzyme
test('Realistic approach: When clicked to show filters, filters are displayed', () => {
    // Arrange
    const wrapper = mount(<Calendar showFilters={false} />)

    // Act
    wrapper.find('button').simulate('click');

    // Assert
    expect(wrapper.text().includes('Choose Filter'));
    // This is how the user will approach this element: by text
})


```

### :thumbsdown: Exemplo Anti-padr√£o: Simulando a realidade com renderiza√ß√£o superficial
```javascript

test('Shallow/mocked approach: When clicked to show filters, filters are displayed', () => {
    // Arrange
    const wrapper = shallow(<Calendar showFilters={false} title='Choose Filter'/>)

    // Act
    wrapper.find('filtersPanel').instance().showFilters();
    // Tap into the internals, bypass the UI and invoke a method. White-box approach

    // Assert
    expect(wrapper.find('Filter').props()).toEqual({title: 'Choose Filter'});
    // what if we change the prop name or don't pass anything relevant?
})

```

</details>

<br/>


## ‚ö™ Ô∏è 3.4 N√£o durma, use o suporte incorporado de frameworks para eventos ass√≠ncronos. Tamb√©m tente acelerar as coisas

:white_check_mark: **Fa√ßa:** Em muitos casos, o tempo de conclus√£o da unidade em teste √© desconhecido (por exemplo, a anima√ß√£o suspende a apar√™ncia do elemento) - nesse caso, evite dormir (por exemplo, setTimeOut) e prefira m√©todos mais determin√≠sticos que a maioria das plataformas fornece. Algumas bibliotecas permitem aguardar opera√ß√µes (por exemplo, [Cypress cy.request('url')](https://docs.cypress.io/guides/references/best-practices.html#Unnecessary-Waiting)), outras fornecem API para esperar como [@testing-library/dom method wait(expect(element))](https://testing-library.com/docs/guide-disappearance). √Äs vezes, uma maneira mais elegante √© esbo√ßar o recurso lento, como a API, por exemplo, e depois que o momento da resposta se torna determin√≠stico, o componente pode ser explicitamente renderizado novamente. Quando, dependendo de algum componente externo que dorme, pode ser √∫til [apressar o rel√≥gio](https://jestjs.io/docs/en/timer-mocks). Dormir √© um padr√£o a ser evitado, porque for√ßa o teste a ser lento ou arriscado (ao esperar por um per√≠odo muito curto). Sempre que dormir e pesquisar for inevit√°vel e n√£o h√° suporte do framework de teste, algumas bibliotecas do NPM, como [wait-for-expect](https://www.npmjs.com/package/wait-for-expect) podem ajudar com uma solu√ß√£o semi-determin√≠stica 
<br/>

‚ùå **Caso contr√°rio:** Ao dormir por um longo tempo, os testes ser√£o uma ordem de magnitude mais lenta. Ao tentar dormir por pequenos n√∫meros, o teste falha quando a unidade em teste n√£o responde em tempo h√°bil. Portanto, tudo se resume a uma troca entre descama√ß√£o e mau desempenho


<br/>

<details><summary>‚úè <b>C√≥digos de Exemplo</b></summary>

<br/>

### :clap: Exemplo Fazendo Certo: API E2E que resolve somente quando as opera√ß√µes ass√≠ncronas s√£o conclu√≠das (Cypress)

![](https://img.shields.io/badge/üîß%20Exemplo%20usando%20React-blue.svg
 "Exemplos com React") ![](https://img.shields.io/badge/üîß%20Exemplo%20usando%20React%20Testing%20Library-blue.svg
 "Exemplos com react-testing-library")

```javascript
// using Cypress
cy.get('#show-products').click()// navigate
cy.wait('@products')// wait for route to appear
// this line will get executed only when the route is ready

```

### :clap: Exemplo Fazendo Certo: Biblioteca de teste que aguarda elementos do DOM

```javascript
// @testing-library/dom
test('movie title appears', async () => {
    // element is initially not present...

    // wait for appearance
    await wait(() => {
        expect(getByText('the lion king')).toBeInTheDocument()
    })

    // wait for appearance and return the element
    const movie = await waitForElement(() => getByText('the lion king'))
})

```

### :thumbsdown: Exemplo Anti-padr√£o: c√≥digo de suspens√£o personalizado
```javascript

test('movie title appears', async () => {
    // element is initially not present...

    // custom wait logic (caution: simplistic, no timeout)
    const interval = setInterval(() => {
        const found = getByText('the lion king');
        if(found){
            clearInterval(interval);
            expect(getByText('the lion king')).toBeInTheDocument();
        }
        
    }, 100);

    // wait for appearance and return the element
    const movie = await waitForElement(() => getByText('the lion king'))
})

```

</details>


<br/>

## ‚ö™ Ô∏è 3.5. Veja como o conte√∫do √© servido na rede

![](https://img.shields.io/badge/üîß%20Exemplo%20usando%20Google%20LightHouse-blue.svg
 "Exemplo com Lighthouse")

‚úÖ **Fa√ßa:** Aplique um monitor ativo que garanta que o carregamento da p√°gina na rede real seja otimizado - isso inclui qualquer preocupa√ß√£o de UX, como carregamento lento da p√°gina ou pacote n√£o minificado. O mercado de ferramentas de inspe√ß√£o n√£o √© curto: ferramentas b√°sicas como [pingdom](https://www.pingdom.com/), AWS CloudWatch, [gcp StackDriver](https://cloud.google.com/monitoring/uptime-checks/) podem ser facilmente configuradas para verificar se o servidor est√° ativo e responde sob um SLA razo√°vel. Isso apenas arranha a superf√≠cie do que pode estar errado; portanto, √© prefer√≠vel optar por ferramentas especializadas em frontend(por exemplo, [lighthouse](https://developers.google.com/web/tools/lighthouse/), [pagespeed](https://developers.google.com/speed/pagespeed/insights/)) e realizar an√°lises mais ricas. O foco deve estar nos sintomas, nas m√©tricas que afetam diretamente o UX, como o tempo de carregamento da p√°gina, [meaningful paint](https://scotch.io/courses/10-web-performance-audit-tips-for-your-next-billion-users-in-2018/fmp-first-meaningful-paint), [ttempo at√© a p√°gina ficar interativa (TTI)](https://calibreapp.com/blog/time-to-interactive/). Al√©m disso, tamb√©m √© poss√≠vel observar causas t√©cnicas, como garantir que o conte√∫do seja compactado, tempo at√© o primeiro byte, otimizar imagens, garantir tamanho razo√°vel de DOM, SSL e muitos outros. √â aconselh√°vel ter esses monitores avan√ßados durante o desenvolvimento, como parte do CI e o mais importante - 24x7 nos servidores de produ√ß√£o/CDN

<br/>

‚ùå **Caso contr√°rio:** Deve ser decepcionante perceber que, ap√≥s um cuidado t√£o grande na cria√ß√£o de uma interface do usu√°rio, testes 100% funcionais passando e empacotamento sofisticado - o UX √© horr√≠vel e lento devido √† configura√ß√£o incorreta da CDN

<br/>

<details><summary>‚úè <b>C√≥digos de Exemplo</b></summary>

### :clap: Exemplo Fazendo Certo: Relat√≥rio de inspe√ß√£o de carregamento de p√°gina do Lighthouse

![](/assets/lighthouse2.png "Relat√≥rio de inspe√ß√£o de carregamento de p√°gina do Lighthouse")


</details>


<br/>

## ‚ö™ Ô∏è 3.6 Esboce recursos escamosos e lentos, como APIs de backend

:white_check_mark: **Fa√ßa:** Ao codificar seus testes convencionais (n√£o os testes E2E), evite envolver qualquer recurso que esteja al√©m de sua responsabilidade e controle como a API de backend e use esbo√ßos (por exemplo, teste duplo). Na pr√°tica, em vez de chamadas de rede reais para APIs, use alguma biblioteca de teste duplo (como [Sinon](https://sinonjs.org/), [Test doubles](https://www.npmjs.com/package/testdouble), etc) para esbo√ßar a resposta da API. O principal benef√≠cio √© evitar falhas - APIs de teste ou preparo, por defini√ß√£o, n√£o s√£o altamente est√°veis ‚Äã‚Äãe, de tempos em tempos, ser√£o reprovados em seus testes, embora SEU componente se comporte perfeitamente (o ambiente de produ√ß√£o n√£o foi projetado para testes e geralmente limita as solicita√ß√µes). Isso permitir√° simular v√°rios comportamentos da API que devem direcionar o comportamento do componente como quando nenhum dado foi encontrado ou o caso em que a API gera um erro. Por √∫ltimo, mas n√£o menos importante, as chamadas de rede desacelerar√£o bastante os testes

<br/>

‚ùå **Caso contr√°rio:** Um teste m√©dio √© executado em n√£o mais do que alguns ms, uma chamada t√≠pica da API dura 100ms>, isso torna cada teste ~ 20x mais lento


<br/>

<details><summary>‚úè <b>C√≥digos de Exemplo</b></summary>

<br/>

### :clap: Exemplo Fazendo Certo: fazendo o esbo√ßo ou interceptando chamadas de API
![](https://img.shields.io/badge/üîß%20Exemplo%20usando%20React-blue.svg
 "Exemplos com React") ![](https://img.shields.io/badge/üîß%20Exemplo%20usando%20Jest-blue.svg
 "Exemplos com react-testing-library")
 
```javascript

// unit under test
export default function ProductsList() { 
    const [products, setProducts] = useState(false)

    const fetchProducts = async() => {
      const products = await axios.get('api/products')
      setProducts(products);
    }

    useEffect(() => {
      fetchProducts();
    }, []);

  return products ? <div>{products}</div> : <div data-testid='no-products-message'>No products</div>
}

// test
test('When no products exist, show the appropriate message', () => {
    // Arrange
    nock("api")
            .get(`/products`)
            .reply(404);

    // Act
    const {getByTestId} = render(<ProductsList/>);

    // Assert
    expect(getByTestId('no-products-message')).toBeTruthy();
});

```

</details>

<br/>

## ‚ö™ Ô∏è 3.7 Tenha muito poucos testes de ponta a ponta que abrangem todo o sistema

:white_check_mark: **Fa√ßa:** Embora o E2E (ponta a ponta) geralmente signifique testes apenas da interface do usu√°rio com um navegador real (consulte o item 3.6), para outros, eles significam testes que abrangem todo o sistema, incluindo o backend real. O √∫ltimo tipo de teste √© altamente valioso, pois cobre erros de integra√ß√£o entre frontend e backend que podem ocorrer devido a um entendimento incorreto do esquema de troca. Eles tamb√©m s√£o um m√©todo eficiente para descobrir problemas de integra√ß√£o de backend para backend (por exemplo, o microsservi√ßo A envia a mensagem errada para o microsservi√ßo B) e at√© mesmo para detectar falhas de implanta√ß√£o - n√£o h√° estruturas de backend para testes E2E que sejam t√£o amig√°veis ‚Äã‚Äãe maduras quanto as estruturas de interface do usu√°rio, como [Cypress](https://www.cypress.io/) e [Pupeteer](https://github.com/GoogleChrome/puppeteer). A desvantagem de tais testes √© o alto custo de configura√ß√£o de um ambiente com tantos componentes e principalmente sua fragilidade - com 50 microsservi√ßos, mesmo se um falhar, todo o E2E falhou. Por esse motivo, devemos usar essa t√©cnica com modera√ß√£o e provavelmente ter de 1 a 10 desses e n√£o mais. Dito isto, mesmo um pequeno n√∫mero de testes E2E provavelmente capturar√° o tipo de problemas para os quais eles s√£o direcionados - falhas de implanta√ß√£o e integra√ß√£o. √â aconselh√°vel execut√°-las em um ambiente de prepara√ß√£o semelhante √† produ√ß√£o

<br/>

‚ùå **Caso contr√°rio:** A interface do usu√°rio pode investir muito em testar sua funcionalidade apenas para perceber muito tarde que a carga √∫til retornada pelo backend (o esquema de dados com o qual a interface do usu√°rio precisa trabalhar) √© muito diferente do esperado

<br/>

## ‚ö™ Ô∏è 3.8 Acelere os testes do E2E reutilizando credenciais de login

:white_check_mark: **Fa√ßa:** Nos testes E2E que envolvem um back-end real e dependem de um token de usu√°rio v√°lido para chamadas de API, n√£o vale a pena isolar o teste para um n√≠vel em que um usu√°rio √© criado e conectado a cada solicita√ß√£o. Em vez disso, efetue login apenas uma vez antes do in√≠cio da execu√ß√£o dos testes (ou seja, antes de tudo), salve o token em algum armazenamento local e reutilize-o nas solicita√ß√µes. Isso parece violar um dos principais princ√≠pios de teste - mantenha o teste aut√¥nomo sem acoplamento de recursos. Embora essa seja uma preocupa√ß√£o v√°lida, nos testes E2E o desempenho √© uma preocupa√ß√£o importante e a cria√ß√£o de 1-3 solicita√ß√µes de API antes de iniciar cada teste individual pode levar a um tempo de execu√ß√£o horr√≠vel. Reutilizar credenciais n√£o significa que os testes precisam agir nos mesmos registros do usu√°rio - se depender de registros do usu√°rio (por exemplo, hist√≥rico de pagamentos do usu√°rio do teste), certifique-se de gerar esses registros como parte do teste e evitar compartilhar sua exist√™ncia com outros testes. Lembre-se tamb√©m de que o backend pode ser falsificado - se seus testes estiverem focados no frontend, pode ser melhor isol√°-lo e esbo√ßar a API de backend (consulte o item 3.6).

<br/>

‚ùå **Caso contr√°rio:** Dado 200 casos de teste e assumindo login=100ms = 20 segundos apenas para efetuar login novamente e novamente

<br/>

<details><summary>‚úè <b>C√≥digos de Exemplo</b></summary>

<br/>

### :clap: Exemplo Fazendo Certo: Fazer login antes de todos e n√£o antes de cada

![](https://img.shields.io/badge/üî®%20Exaeplo%20usando%20Cypress-blue.svg
 "Usando Cypress para ilustrar a idea")

```javascript
let authenticationToken;

// happens before ALL tests run
before(() => {
  cy.request('POST', 'http://localhost:3000/login', {
    username: Cypress.env('username'),
    password: Cypress.env('password'),
  })
  .its('body')
  .then((responseFromLogin) => {
    authenticationToken = responseFromLogin.token;
  })
})

// happens before EACH test
beforeEach(setUser => () {
  cy.visit('/home', {
    onBeforeLoad (win) {
      win.localStorage.setItem('token', JSON.stringify(authenticationToken))
    },
  })
})

```

</details>




<br/>

## ‚ö™ Ô∏è 3.9 Fa√ßa um teste de fuma√ßa E2E que viaja pelo mapa do site

:white_check_mark: **Fa√ßa:** Para monitoramento de produ√ß√£o e verifica√ß√£o de integridade do tempo de desenvolvimento, execute um √∫nico teste E2E que visite todas/a maioria das p√°ginas do site e garanta que nenhuma quebre. Esse tipo de teste traz um √≥timo retorno do investimento, pois √© muito f√°cil de escrever e manter, mas pode detectar qualquer tipo de falha, incluindo problemas funcionais, de rede e de implanta√ß√£o. Outros estilos de verifica√ß√£o de fuma√ßa e sanidade n√£o s√£o t√£o confi√°veis ‚Äã‚Äãe exaustivos - algumas equipes de opera√ß√µes apenas fazem ping na p√°gina inicial (produ√ß√£o) ou desenvolvedores que executam muitos testes de integra√ß√£o os quais n√£o descobrem problemas de empacotamento e de navegador. Nem precisa dizer que o teste de fuma√ßa n√£o substitui os testes funcionais, apenas visa servir como um detector de fuma√ßa r√°pido

<br/>

‚ùå **Caso contr√°rio:** Tudo pode parecer perfeito, todos os testes s√£o aprovados, a verifica√ß√£o de integridade da produ√ß√£o tamb√©m √© positiva, mas o componente Payment teve algum problema de embalagem e apenas a rota /Payment n√£o est√° sendo processada


<br/>

<details><summary>‚úè <b>C√≥digos de Exemplo</b></summary>

<br/>

### :clap: Exemplo Fazendo Certo: Fuma√ßa viajando por todas as p√°ginas
![](https://img.shields.io/badge/üî®%20Example%20using%20Cypress-blue.svg
 "Using Cypress to illustrate the idea")
```javascript
it('When doing smoke testing over all page, should load them all successfully', () => {
    // exemplified using Cypress but can be implemented easily
    // using any E2E suite
    cy.visit('https://mysite.com/home');
    cy.contains('Home');
    cy.contains('https://mysite.com/Login');
    cy.contains('Login');
    cy.contains('https://mysite.com/About');
    cy.contains('About');
  })
```

</details>


<br/>

## ‚ö™ Ô∏è 3.10 Expor os testes como um documento colaborativo vivo

:white_check_mark: **Fa√ßa:** Al√©m de aumentar a confiabilidade do aplicativo, os testes trazem outra oportunidade atraente para a mesa - servem como documenta√ß√£o viva do aplicativo. Como os testes falam inerentemente em uma linguagem menos t√©cnica e de produto/UX, usando as ferramentas certas, eles podem servir como um artefato de comunica√ß√£o que alinha muito os colegas - desenvolvedores e seus clientes. Por exemplo, algumas estruturas permitem expressar o fluxo e as expectativas (ou seja, plano de testes) usando uma linguagem leg√≠vel por humanos, para que qualquer parte interessada, incluindo gerentes de produto, possa ler, aprovar e colaborar nos testes que acabaram de se tornar o documento de requisitos din√¢micos. Essa t√©cnica tamb√©m est√° sendo chamada de 'teste de aceita√ß√£o', pois permite ao cliente definir seus crit√©rios de aceita√ß√£o em linguagem simples. Isso √© [BDD (teste orientado ao comportamento)](https://en.wikipedia.org/wiki/Behavior-driven_development) na sua forma mais pura. Um dos frameworks populares que permitem isso √© [Cucumber que tem um sabor de JavaScript](https://github.com/cucumber/cucumber-js), veja o exemplo abaixo. Outra oportunidade semelhante, por√©m diferente, [StoryBook](https://storybook.js.org/), permite expor componentes da interface do usu√°rio como um cat√°logo gr√°fico, onde √© poss√≠vel percorrer os v√°rios estados de cada componente (por exemplo. renderizar uma grid sem filtros, renderizar essa grid com v√°rias linhas ou sem nenhuma, etc.), ver como ele se parece e como acionar esse estado - isso tamb√©m pode atrair as pessoas do produto, mas serve principalmente como documento ativo para desenvolvedores que consomem esses componentes.

‚ùå **Otherwise:** Depois de investir muitos recursos em testes, √© uma pena n√£o alavancar esse investimento e obter grande valor


<br/>

<details><summary>‚úè <b>C√≥digos de Exemplo</b></summary>

<br/>

### :clap: Exemplo Fazendo Certo: Descrevendo testes em linguagem humana usando cucumber-js

![](https://img.shields.io/badge/üî®%20Exemplo%20usando%20Cocumber-blue.svg  "Exemplos usando Cucumber")
```javascript
// this is how one can describe tests using cucumber: plain language that allows anyone to understand and collaborate

Feature: Twitter new tweet
 
  I want to tweet something in Twitter
  
  @focus
  Scenario: Tweeting from the home page
    Given I open Twitter home
    Given I click on "New tweet" button
    Given I type "Hello followers!" in the textbox 
    Given I click on "Submit" button
    Then I see message "Tweet saved"
    
```

### :clap: Exemplo Fazendo Certo: Visualizando nossos componentes, seus v√°rios estados e entradas usando o Storybook
![](https://img.shields.io/badge/üî®%20Example%20using%20StoryBook-blue.svg "Usando StoryBook")


</details>




## ‚ö™ Ô∏è 3.11 Detecte problemas visuais com ferramentas automatizadas


:white_check_mark: **Fa√ßa:** Configure ferramentas automatizadas para capturar a tela da interface do usu√°rio quando altera√ß√µes forem apresentadas e detectar problemas visuais, como sobreposi√ß√£o ou quebra de conte√∫do. Isso garante que n√£o apenas os dados corretos sejam preparados, mas tamb√©m que o usu√°rio possa v√™-los convenientemente. Essa t√©cnica n√£o √© amplamente adotada, nossa mentalidade de teste se inclina para testes funcionais, mas √© o visual que o usu√°rio experimenta e, com tantos tipos de dispositivos, √© muito f√°cil ignorar alguns erros desagrad√°veis ‚Äã‚Äãda interface do usu√°rio. Algumas ferramentas gratuitas podem fornecer o b√°sico - gerar e salvar capturas de tela para a inspe√ß√£o dos olhos humanos. Embora essa abordagem possa ser suficiente para aplicativos pequenos, ela √© falha como qualquer outro teste manual que exige m√£o de obra humana sempre que algo muda. Por outro lado, √© bastante desafiador detectar problemas de interface do usu√°rio automaticamente devido √† falta de defini√ß√£o clara - √© aqui que o campo de 'Regress√£o Visual' entra em cena e resolve esse quebra-cabe√ßa comparando a interface antiga com as altera√ß√µes mais recentes e detectando diferen√ßas. Algumas ferramentas OSS/gratuitas podem fornecer algumas dessas funcionalidades (por exemplo, [wraith](https://github.com/BBC-News/wraith), [PhantomCSS]([https://github.com/HuddleEng/PhantomCSS](https://github.com/HuddleEng/PhantomCSS)) mas pode cobrar um tempo significativo de configura√ß√£o. A linha comercial de ferramentas (por exemplo [Applitools](https://applitools.com/), [Percy.io](https://percy.io/)) d√° um passo adiante ao suavizar a instala√ß√£o e incluir recursos avan√ßados, como interface de gerenciamento, alerta, captura inteligente, eliminando o 'ru√≠do visual' (por exemplo, an√∫ncios, anima√ß√µes) e at√© mesmo a an√°lise de causa raiz das altera√ß√µes no DOM/css que levaram ao problema

<br/>

‚ùå **Caso contr√°rio:** Qu√£o boa √© uma p√°gina de conte√∫do que exibe √≥timo conte√∫do (100% nos testes aprovados), carrega instantaneamente, mas metade da √°rea de conte√∫do est√° oculta?


<br/>

<details><summary>‚úè <b>C√≥digos de Exemplo</b></summary>

<br/>

### :thumbsdown: Exemplo Anti-padr√£o: Uma regress√£o visual t√≠pica - conte√∫do correto que √© mal servido

![alt text](assets/amazon-visual-regression.jpeg "Quebras de p√°gina na Amazon")

<br/>


### :clap: Exemplo Fazendo Certo: Configurando o wraith para capturar e comparar instant√¢neos da UI

![](https://img.shields.io/badge/üî®%20Exemplo%20usando%20Wraith-blue.svg
 "Usando Cypress para ilustrar a idea")

```
‚Äã# Add as many domains as necessary. Key will act as a label‚Äã

domains:
  english: "http://www.mysite.com"‚Äã

‚Äã# Type screen widths below, here are a couple of examples‚Äã

screen_widths:

  - 600‚Äã
  - 768‚Äã
  - 1024‚Äã
  - 1280‚Äã


‚Äã# Type page URL paths below, here are a couple of examples‚Äã
paths:
  about:
    path: /about
    selector: '.about'‚Äã
  subscribe:
      selector: '.subscribe'‚Äã
    path: /subscribe
```

### :clap: Exemplo Fazendo Certo: Usando Applitools para obter compara√ß√£o de captura instant√¢nea e outros recursos avan√ßados

![](https://img.shields.io/badge/üî®%20Exemplo%20usando%20AppliTools-blue.svg
 "Usando Cypress to para ilustrar a idea") ![](https://img.shields.io/badge/üî®%20Exemplo%20usando%20Cypress-blue.svg
 "Usando Cypress para illustrar idea")

```javascript
import  *  as todoPage from  '../page-objects/todo-page';

describe('visual validation',  ()  =>  {

before(()  =>  todoPage.navigate());

beforeEach(()  =>  cy.eyesOpen({ appName:  'TAU TodoMVC'  }));

afterEach(()  =>  cy.eyesClose());

  

it('should look good',  ()  =>  {

cy.eyesCheckWindow('empty todo list');

  

todoPage.addTodo('Clean room');

  

todoPage.addTodo('Learn javascript');

  

cy.eyesCheckWindow('two todos');

  

todoPage.toggleTodo(0);

  

cy.eyesCheckWindow('mark as completed');

});

});
```




</details>



<br/><br/>

  
# Section 4Ô∏è‚É£: Measuring Test Effectiveness

<br/><br/>

## ‚ö™ Ô∏è 4.1 Get enough coverage for being confident, ~80% seems to be the lucky number

:white_check_mark: **Do:** The purpose of testing is to get enough confidence for moving fast, obviously the more code is tested the more confident the team can be. Coverage is a measure of how many code lines (and branches, statements, etc) are being reached by the tests. So how much is enough? 10‚Äì30% is obviously too low to get any sense about the build correctness, on the other side 100% is very expensive and might shift your focus from the critical paths to the exotic corners of the code. The long answer is that it depends on many factors like the type of application‚Ää‚Äî‚Ääif you‚Äôre building the next generation of Airbus A380 than 100% is a must, for a cartoon pictures website 50% might be too much. Although most of the testing enthusiasts claim that the right coverage threshold is contextual, most of them also mention the number 80% as a thumb of a rule ([Fowler: ‚Äúin the upper 80s or 90s‚Äù](https://martinfowler.com/bliki/TestCoverage.html)) that presumably should satisfy most of the applications.

Implementation tips: You may want to configure your continuous integration (CI) to have a coverage threshold ([Jest link](https://jestjs.io/docs/en/configuration.html#collectcoverage-boolean)) and stop a build that doesn‚Äôt stand to this standard (it‚Äôs also possible to configure threshold per component, see code example below). On top of this, consider detecting build coverage decrease (when a newly committed code has less coverage)‚Ää‚Äî‚Ääthis will push developers raising or at least preserving the amount of tested code. All that said, coverage is only one measure, a quantitative based one, that is not enough to tell the robustness of your testing. And it can also be fooled as illustrated in the next bullets

<br/>


‚ùå **Otherwise:**  Confidence and numbers go hand in hand, without really knowing that you tested most of the system‚Ää‚Äî‚Ääthere will also be some fear. and fear will slow you down


<br/>

<details><summary>‚úè <b>Code Examples</b></summary>

<br/>

### :clap: Example: A typical coverage report
![alt text](assets/bp-18-yoni-goldberg-code-coverage.png "A typical coverage report")

<br/>

### :clap: Doing It Right Example: Setting up coverage per component (using Jest)

![](https://img.shields.io/badge/üî®%20Example%20using%20Jest-blue.svg
 "Using Cypress to illustrate the idea")

![alt text](assets/bp-18-code-coverage2.jpeg "Setting up coverage per component (using Jest)

</details>



<br/><br/>

## ‚ö™ Ô∏è 4.2 Inspect coverage reports to detect untested areas and other oddities

:white_check_mark: **Do:** Some issues sneak just under the radar and are really hard to find using traditional tools. These are not really bugs but more of surprising application behavior that might have a severe impact. For example, often some code areas are never or rarely being invoked‚Ää‚Äî‚Ääyou thought that the ‚ÄòPricingCalculator‚Äô class is always setting the product price but it turns out it is actually never invoked although we have 10000 products in DB and many sales‚Ä¶ Code coverage reports help you realize whether the application behaves the way you believe it does. Other than that, it can also highlight which types of code is not tested‚Ää‚Äî‚Ääbeing informed that 80% of the code is tested doesn‚Äôt tell whether the critical parts are covered. Generating reports is easy‚Ää‚Äî‚Ääjust run your app in production or during testing with coverage tracking and then see colorful reports that highlight how frequent each code area is invoked. If you take your time to glimpse into this data‚Ää‚Äî‚Ääyou might find some gotchas
<br/>


‚ùå **Otherwise:** If you don‚Äôt know which parts of your code are left un-tested, you don‚Äôt know where the issues might come from


<br/>

<details><summary>‚úè <b>Code Examples</b></summary>

<br/>

### :thumbsdown: Anti-Pattern Example: What‚Äôs wrong with this coverage report? based on a real-world scenario where we tracked our application usage in QA and find out interesting login patterns (Hint: the amount of login failures is non-proportional, something is clearly wrong. Finally it turned out that some frontend bug keeps hitting the backend login API)

![alt text](assets/bp-19-coverage-yoni-goldberg-nodejs-consultant.png "What‚Äôs wrong with this coverage report? based on a real-world scenario where we tracked our application usage in QA and find out interesting login patterns (Hint: the amount of login failures is non-proportional, something is clearly wrong. Finally it turned out that some frontend bug keeps hitting the backend login API)

</details>


<br/><br/>

## ‚ö™ Ô∏è 4.3 Measure logical coverage using mutation testing

:white_check_mark: **Do:**  The Traditional Coverage metric often lies: It may show you 100% code coverage, but none of your functions, even not one, return the right response. How come? it simply measures over which lines of code the test visited, but it doesn‚Äôt check if the tests actually tested anything‚Ää‚Äî‚Ääasserted for the right response. Like someone who‚Äôs traveling for business and showing his passport stamps‚Ää‚Äî‚Ääthis doesn‚Äôt prove any work done, only that he visited few airports and hotels.

Mutation-based testing is here to help by measuring the amount of code that was actually TESTED not just VISITED. [Stryker](https://stryker-mutator.io/) is a JavaScript library for mutation testing and the implementation is really neat:

(1) it intentionally changes the code and ‚Äúplants bugs‚Äù. For example the code newOrder.price===0 becomes newOrder.price!=0. This ‚Äúbugs‚Äù are called mutations

(2) it runs the tests, if all succeed then we have a problem‚Ää‚Äî‚Ääthe tests didn‚Äôt serve their purpose of discovering bugs, the mutations are so-called survived. If the tests failed, then great, the mutations were killed.

Knowing that all or most of the mutations were killed gives much higher confidence than traditional coverage and the setup time is similar
<br/>


‚ùå **Otherwise:** You‚Äôll be fooled to believe that 85% coverage means your test will detect bugs in 85% of your code

<br/>

<details><summary>‚úè <b>Code Examples</b></summary>

<br/>

### :thumbsdown: Anti Pattern Example: 100% coverage, 0% testing

![](https://img.shields.io/badge/üî®%20Example%20using%20Stryker-blue.svg
 "Using Cypress to illustrate the idea")
```javascript
function addNewOrder(newOrder) {
    logger.log(`Adding new order ${newOrder}`);
    DB.save(newOrder);
    Mailer.sendMail(newOrder.assignee, `A new order was places ${newOrder}`);

    return {approved: true};
}

it("Test addNewOrder, don't use such test names", () => {
    addNewOrder({asignee: "John@mailer.com",price: 120});
});//Triggers 100% code coverage, but it doesn't check anything

```
<br/>

### :clap: Doing It Right Example: Stryker reports, a tool for mutation testing, detects and counts the amount of code that is not tested (Mutations)

![alt text](assets/bp-20-yoni-goldberg-mutation-testing.jpeg "Stryker reports, a tool for mutation testing, detects and counts the amount of code that is not tested (Mutations)")

</details>



<br/><br/>

## ‚ö™ Ô∏è4.4 Preventing test code issues with Test linters

:white_check_mark: **Do:**  A set of ESLint plugins were built specifically for inspecting the tests code patterns and discover issues. For example, [eslint-plugin-mocha](https://www.npmjs.com/package/eslint-plugin-mocha) will warn when a test is written at the global level (not a son of a describe() statement) or when tests are [skipped](https://mochajs.org/#inclusive-tests) which might lead to a false belief that all tests are passing. Similarly, [eslint-plugin-jest](https://github.com/jest-community/eslint-plugin-jest) can, for example, warn when a test has no assertions at all (not checking anything)

<br/>


‚ùå **Otherwise:** Seeing 90% code coverage and 100% green tests will make your face wear a big smile only until you realize that many tests aren‚Äôt asserting for anything and many test suites were just skipped. Hopefully, you didn‚Äôt deploy anything based on this false observation


<br/>
<details><summary>‚úè <b>Code Examples</b></summary>

<br/>

### :thumbsdown: Anti Pattern Example: A test case full of errors, luckily all are caught by Linters

```javascript
describe("Too short description", () => {
  const userToken = userService.getDefaultToken() // *error:no-setup-in-describe, use hooks (sparingly) instead
  it("Some description", () => {});//* error: valid-test-description. Must include the word "Should" + at least 5 words
});

it.skip("Test name", () => {// *error:no-skipped-tests, error:error:no-global-tests. Put tests only under describe or suite
  expect("somevalue"); // error:no-assert
});

it("Test name", () => {*//error:no-identical-title. Assign unique titles to tests
});
```

</details>

<br/><br/>

  
# Section 5Ô∏è‚É£: CI and Other Quality Measures

<br/><br/>

## ‚ö™ Ô∏è 5.1 Enrich your linters and abort builds that have linting issues

:white_check_mark: **Do:**  Linters are a free lunch, with 5 min setup you get for free an auto-pilot guarding your code and catching significant issue as you type. Gone are the days where linting was about cosmetics (no semi-colons!). Nowadays, Linters can catch severe issues like errors that are not thrown correctly and losing information. On top of your basic set of rules (like [ESLint standard](https://www.npmjs.com/package/eslint-plugin-standard) or [Airbnb style](https://www.npmjs.com/package/eslint-config-airbnb)), consider including some specializing Linters like [eslint-plugin-chai-expect](https://www.npmjs.com/package/eslint-plugin-chai-expect) that can discover tests without assertions, [eslint-plugin-promise](https://www.npmjs.com/package/eslint-plugin-promise?activeTab=readme) can discover promises with no resolve (your code will never continue), [eslint-plugin-security](https://www.npmjs.com/package/eslint-plugin-security?activeTab=readme) which can discover eager regex expressions that might get used for DOS attacks, and [eslint-plugin-you-dont-need-lodash-underscore](https://www.npmjs.com/package/eslint-plugin-you-dont-need-lodash-underscore) is capable of alarming when the code uses utility library methods that are part of the V8 core methods like Lodash._map(‚Ä¶)
<br/>


‚ùå **Otherwise:** Consider a rainy day where your production keeps crashing but the logs don‚Äôt display the error stack trace. What happened? Your code mistakenly threw a non-error object and the stack trace was lost, a good reason for banging your head against a brick wall. A 5min linter setup could detect this TYPO and save your day


<br/>

<details><summary>‚úè <b>Code Examples</b></summary>

<br/>

### :thumbsdown: Anti Pattern Example: The wrong Error object is thrown mistakenly, no stack-trace will appear for this error. Luckily, ESLint catches the next production bug
![alt text](assets/bp-21-yoni-goldberg-eslint.jpeg "The wrong Error object is thrown mistakenly, no stack-trace will appear for this error. Luckily, ESLint catches the next production bug")

</details>




<br/><br/>

# ‚ö™ Ô∏è 5.2 Shorten the feedback loop with local developer-CI

:white_check_mark: **Do:**   Using a CI with shiny quality inspections like testing, linting, vulnerabilities check, etc? Help developers run this pipeline also locally to solicit instant feedback and shorten the [feedback loop](https://www.gocd.org/2016/03/15/are-you-ready-for-continuous-delivery-part-2-feedback-loops/). Why? an efficient testing process constitutes many and iterative loops: (1) try-outs -> (2) feedback -> (3) refactor. The faster the feedback is, the more improvement iterations a developer can perform per-module and perfect the results. On the flip, when the feedback is late to come fewer improvement iterations could be packed into a single day, the team might already move forward to another topic/task/module and might not be up for refining that module.

Practically, some CI vendors (Example: [CircleCI load CLI](https://circleci.com/docs/2.0/local-cli/)) allow running the pipeline locally. Some commercial tools like [wallaby provide highly-valuable & testing insights](https://wallabyjs.com/) as a developer prototype (no affiliation). Alternatively, you may just add npm script to package.json that runs all the quality commands (e.g. test, lint, vulnerabilities)‚Ää‚Äî‚Ääuse tools like [concurrently](https://www.npmjs.com/package/concurrently) for parallelization and non-zero exit code if one of the tools failed. Now the developer should just invoke one command‚Ää‚Äî‚Ääe.g. ‚Äònpm run quality‚Äô‚Ää‚Äî‚Ääto get instant feedback. Consider also aborting a commit if the quality check failed using a githook ([husky can help](https://github.com/typicode/husky))
<br/>


‚ùå **Otherwise:** When the quality results arrive the day after the code, testing doesn‚Äôt become a fluent part of development rather an after the fact formal artifact


<br/>

<details><summary>‚úè <b>Code Examples</b></summary>

<br/>

### :clap:  Doing It Right Example: npm scripts that perform code quality inspection, all are run in parallel on demand or when a developer is trying to push new code
```javascript
"scripts": {
    "inspect:sanity-testing": "mocha **/**--test.js --grep \"sanity\"",
    "inspect:lint": "eslint .",
    "inspect:vulnerabilities": "npm audit",
    "inspect:license": "license-checker --failOn GPLv2",
    "inspect:complexity": "plato .",

    "inspect:all": "concurrently -c \"bgBlue.bold,bgMagenta.bold,yellow\" \"npm:inspect:quick-testing\" \"npm:inspect:lint\" \"npm:inspect:vulnerabilities\" \"npm:inspect:license\""
  },
  "husky": {
    "hooks": {
      "precommit": "npm run inspect:all",
      "prepush": "npm run inspect:all"
    }
}

```

</details>




<br/><br/>

# ‚ö™ Ô∏è5.3 Perform e2e testing over a true production-mirror

:white_check_mark: **Do:**   End to end (e2e) testing are the main challenge of every CI pipeline‚Ää‚Äî‚Ääcreating an identical ephemeral production mirror on the fly with all the related cloud services can be tedious and expensive. Finding the best compromise is your game: [Docker-compose](https://serverless.com/) allows crafting isolated dockerized environment with identical containers using a single plain text file but the backing technology (e.g. networking, deployment model) is different from real-world productions. You may combine it with [‚ÄòAWS Local‚Äô](https://github.com/localstack/localstack) to work with a stub of the real AWS services. If you went [serverless](https://serverless.com/) multiple frameworks like serverless and [AWS SAM](https://docs.aws.amazon.com/lambda/latest/dg/serverless_app.html) allows the local invocation of Faas code.

The huge Kubernetes eco-system is yet to formalize a standard convenient tool for local and CI-mirroring though many new tools are launched frequently. One approach is running a ‚Äòminimized-Kubernetes‚Äô using tools like [Minikube](https://kubernetes.io/docs/setup/minikube/) and [MicroK8s](https://microk8s.io/) which resemble the real thing only come with less overhead. Another approach is testing over a remote ‚Äòreal-Kubernetes‚Äô, some CI providers (e.g. [Codefresh](https://codefresh.io/)) has native integration with Kubernetes environment and make it easy to run the CI pipeline over the real thing, others allow custom scripting against a remote Kubernetes.
<br/>


‚ùå **Otherwise:** Using different technologies for production and testing demands maintaining two deployment models and keeps the developers and the ops team separated


<br/>

<details><summary>‚úè <b>Code Examples</b></summary>

<br/>

### :clap:  Example: a CI pipeline that generates Kubernetes cluster on the fly <a href="https://container-solutions.com/dynamic-environments-kubernetes/" data-href="https://container-solutions.com/dynamic-environments-kubernetes/" class="markup--anchor markup--p-anchor" rel="noopener nofollow" target="_blank">([Credit: Dynamic-environments Kubernetes](https://container-solutions.com/dynamic-environments-kubernetes/))</a>

<pre name="38d9" id="38d9" class="graf graf--pre graf-after--p">deploy:<br>stage: deploy<br>image: registry.gitlab.com/gitlab-examples/kubernetes-deploy<br>script:<br>- ./configureCluster.sh $KUBE_CA_PEM_FILE $KUBE_URL $KUBE_TOKEN<br>- kubectl create ns $NAMESPACE<br>- kubectl create secret -n $NAMESPACE docker-registry gitlab-registry --docker-server="$CI_REGISTRY" --docker-username="$CI_REGISTRY_USER" --docker-password="$CI_REGISTRY_PASSWORD" --docker-email="$GITLAB_USER_EMAIL"<br>- mkdir .generated<br>- echo "$CI_BUILD_REF_NAME-$CI_BUILD_REF"<br>- sed -e "s/TAG/$CI_BUILD_REF_NAME-$CI_BUILD_REF/g" templates/deals.yaml | tee ".generated/deals.yaml"<br>- kubectl apply --namespace $NAMESPACE -f .generated/deals.yaml<br>- kubectl apply --namespace $NAMESPACE -f templates/my-sock-shop.yaml<br>environment:<br>name: test-for-ci</pre>

</details>





<br/><br/>

## ‚ö™ Ô∏è5.4 Parallelize test execution
:white_check_mark: **Do:**    When done right, testing is your 24/7 friend providing almost instant feedback. In practice, executing 500 CPU-bounded unit test on a single thread can take too long. Luckily, modern test runners and CI platforms (like [Jest](https://github.com/facebook/jest), [AVA](https://github.com/avajs/ava) and [Mocha extensions](https://github.com/yandex/mocha-parallel-tests)) can parallelize the test into multiple processes and achieve significant improvement in feedback time. Some CI vendors do also parallelize tests across containers (!) which shortens the feedback loop even further. Whether locally over multiple processes, or over some cloud CLI using multiple machines‚Ää‚Äî‚Ääparallelizing demand keeping the tests autonomous as each might run on different processes


‚ùå **Otherwise:** Getting test results 1 hour long after pushing new code, as you already code the next features, is a great recipe for making testing less relevant


<br/>

<details><summary>‚úè <b>Code Examples</b></summary>

<br/>

### :clap: Doing It Right Example: Mocha parallel & Jest easily outrun the traditional Mocha thanks to testing parallelization ([Credit: JavaScript Test-Runners Benchmark](https://medium.com/dailyjs/javascript-test-runners-benchmark-3a78d4117b4))
![alt text](assets/bp-24-yonigoldberg-jest-parallel.png "Mocha parallel & Jest easily outrun the traditional Mocha thanks to testing parallelization (Credit: JavaScript Test-Runners Benchmark)")

</details>




<br/><br/>

## ‚ö™ Ô∏è5.5 Stay away from legal issues using license and plagiarism check
:white_check_mark: **Do:**    Licensing and plagiarism issues are probably not your main concern right now, but why not tick this box as well in 10 minutes? A bunch of npm packages like [license check](https://www.npmjs.com/package/license-checker) and [plagiarism check](https://www.npmjs.com/package/plagiarism-checker) (commercial with free plan) can be easily baked into your CI pipeline and inspect for sorrows like dependencies with restrictive licenses or code that was copy-pasted from Stackoverflow and apparently violates some copyrights

‚ùå **Otherwise:** Unintentionally, developers might use packages with inappropriate licenses or copy paste commercial code and run into legal issues


<br/>

<details><summary>‚úè <b>Code Examples</b></summary>

<br/>

### :clap: Doing It Right Example:
```javascript
//install license-checker in your CI environment or also locally
npm install -g license-checker

//ask it to scan all licenses and fail with exit code other than 0 if it found unauthorized license. The CI system should catch this failure and stop the build
license-checker --summary --failOn BSD

```

<br/>

![alt text](assets/bp-25-nodejs-licsense.png)


</details>



<br/><br/>

## ‚ö™ Ô∏è5.6 Constantly inspect for vulnerable dependencies
:white_check_mark: **Do:**    Licensing and plagiarism issues are probably not your main concern right now, but why not tick this box as well in 10 minutes? A bunch of npm packages like license check and plagiarism check (commercial with free plan) can be easily baked into your CI pipeline and inspect for sorrows like dependencies with restrictive licenses or code that was copy-pasted from Stackoverflow and apparently violates some copyrights
<br/>


‚ùå **Otherwise:** Even the most reputable dependencies such as Express have known vulnerabilities. This can get easily tamed using community tools such as [npm audit](https://docs.npmjs.com/getting-started/running-a-security-audit), or commercial tools like [snyk](https://snyk.io/) (offer also a free community version). Both can be invoked from your CI on every build


<br/>

<details><summary>‚úè <b>Code Examples</b></summary>

<br/>

### :clap: Example: NPM Audit result
![alt text](assets/bp-26-npm-audit-snyk.png "NPM Audit result")

</details>




<br/><br/>

## ‚ö™ Ô∏è5.7 Automate dependency updates
:white_check_mark: **Do:**   Yarn and npm latest introduction of package-lock.json introduced a serious challenge (the road to hell is paved with good intentions)‚Ää‚Äî‚Ääby default now, packages are no longer getting updates. Even a team running many fresh deployments with ‚Äònpm install‚Äô & ‚Äònpm update‚Äô won‚Äôt get any new updates. This leads to subpar dependent packages versions at best or to vulnerable code at worst. Teams now rely on developers goodwill and memory to manually update the package.json or use tools [like ncu](https://www.npmjs.com/package/npm-check-updates) manually. A more reliable way could be to automate the process of getting the most reliable dependency versions, though there are no silver bullet solutions yet there are two possible automation roads:

(1) CI can fail builds that have obsolete dependencies‚Ää‚Äî‚Ääusing tools like [‚Äònpm outdated‚Äô](https://docs.npmjs.com/cli/outdated) or ‚Äònpm-check-updates (ncu)‚Äô . Doing so will enforce developers to update dependencies.

(2) Use commercial tools that scan the code and automatically send pull requests with updated dependencies. One interesting question remaining is what should be the dependency update policy‚Ää‚Äî‚Ääupdating on every patch generates too many overhead, updating right when a major is released might point to an unstable version (many packages found vulnerable on the very first days after being released, [see the](https://nodesource.com/blog/a-high-level-post-mortem-of-the-eslint-scope-security-incident/) eslint-scope incident).

An efficient update policy may allow some ‚Äòvesting period‚Äô‚Ää‚Äî‚Äälet the code lag behind the @latest for some time and versions before considering the local copy as obsolete (e.g. local version is 1.3.1 and repository version is 1.3.8)
<br/>


‚ùå **Otherwise:** Your production will run packages that have been explicitly tagged by their author as risky


<br/>

<details><summary>‚úè <b>Code Examples</b></summary>

<br/>

### :clap:  Example: [ncu](https://www.npmjs.com/package/npm-check-updates) can be used manually or within a CI pipeline to detect to which extent the code lag behind the latest versions
![alt text](assets/bp-27-yoni-goldberg-npm.png "Nncu can be used manually or within a CI pipeline to detect to which extent the code lag behind the latest versions")


</details>


<br/><br/>

## ‚ö™ Ô∏è 5.8 Other, non-Node related, CI tips
:white_check_mark: **Do:**    This post is focused on testing advice that is related to, or at least can be exemplified with Node JS. This bullet, however, groups few non-Node related tips that are well-known

 <ol class="postList"><li name="e3e4" id="e3e4" class="graf graf--li graf-after--p">Use a declarative syntax. This is the only option for most vendors but older versions of Jenkins allows using code or UI</li><li name="1fdc" id="1fdc" class="graf graf--li graf-after--li">Opt for a vendor that has native Docker support</li><li name="edcd" id="edcd" class="graf graf--li graf-after--li">Fail early, run your fastest tests first. Create a ‚ÄòSmoke testing‚Äô step/milestone that groups multiple fast inspections (e.g. linting, unit tests) and provide snappy feedback to the code committer</li><li name="0375" id="0375" class="graf graf--li graf-after--li">Make it easy to skim-through all build artifacts including test reports, coverage reports, mutation reports, logs, etc</li><li name="df82" id="df82" class="graf graf--li graf-after--li">Create multiple pipelines/jobs for each event, reuse steps between them. For example, configure a job for feature branch commits and a different one for master PR. Let each reuse logic using shared steps (most vendors provide some mechanism for code reuse</li><li name="19b0" id="19b0" class="graf graf--li graf-after--li">Never embed secrets in a job declaration, grab them from a secret store or from the job‚Äôs configuration</li><li name="b70d" id="b70d" class="graf graf--li graf-after--li">Explicitly bump version in a release build or at least ensure the developer did so</li><li name="957c" id="957c" class="graf graf--li graf-after--li">Build only once and perform all the inspections over the single build artifact (e.g. Docker image)</li><li name="339b" id="339b" class="graf graf--li graf-after--li">Test in an ephemeral environment that doesn‚Äôt drift state between builds. Caching node_modules might be the only exception</li></ol>
<br/>


‚ùå **Otherwise:** You‚Äòll miss years of wisdom

<br/><br/>

## ‚ö™ Ô∏è 5.9 Build matrix: Run the same CI steps using multiple Node versions
:white_check_mark: **Do:** Quality checking is about serendipity, the more ground you cover the luckier you get in detecting issues early. When developing reusable packages or running a multi-customer production with various configuration and Node versions, the CI must run the pipeline of tests over all the permutations of configurations. For example, assuming we use MySQL for some customers and Postgres for others‚Ää‚Äî‚Ääsome CI vendors support a feature called ‚ÄòMatrix‚Äô which allow running the suit of testing against all permutations of MySQL, Postgres and multiple Node version like 8, 9 and 10. This is done using configuration only without any additional effort (assuming you have testing or any other quality checks). Other CIs who doesn‚Äôt support Matrix might have extensions or tweaks to allow that
<br/>


‚ùå **Otherwise:** So after doing all that hard work of writing testing are we going to let bugs sneak in only because of configuration issues?


<br/>

<details><summary>‚úè <b>Code Examples</b></summary>

<br/>

### :clap:   Example: Using Travis (CI vendor) build definition to run the same test over multiple Node versions
<pre name="f909" id="f909" class="graf graf--pre graf-after--p">language: node_js<br>node_js:<br>  - "7"<br>  - "6"<br>  - "5"<br>  - "4"<br>install:<br>  - npm install<br>script:<br>  - npm run test</pre>
</details>

<br/><br/>

# Team



## Yoni Goldberg

<br/>
<img width="480px" src="assets/yoni-goldberg.jpg"/>
<br/>

**Role:** Writer

**About:** I'm an independent consultant who works with 500 fortune corporates and garage startups on polishing their JS & Node.js applications. More than any other topic I'm fascinated by and aims to master the art of testing. I'm also the author of [Node.js Best Practices](https://github.com/goldbergyoni/nodebestpractices)

<br/>

**Workshop:** üë®‚Äçüè´ Want to learn all these practices and techniques at your offices (Europe & USA)? [Register here for my testing workshop](https://testjavascript.com/)
<br/>

**Follow:**

* [üê¶ Twitter](https://twitter.com/goldbergyoni/)
* [üìû Contact](https://testjavascript.com/contact-2/)
* [‚úâÔ∏è Newsletter](https://testjavascript.com/newsletter//)

<br/>
<hr/>
<br/>


##  [Bruno Scheufler](https://github.com/BrunoScheufler)

**Role:** Tech reviewer and advisor

Took care to revise, improve, lint and polish all the texts 

**About:** full-stack web engineer, Node.js & GraphQL enthusiast
<hr/>
<br/>

## [Ido Richter](https://github.com/idori)

**Role:** Concept, design and great advice

**About:** A savvy frontend developer, CSS expert and emojis freak
